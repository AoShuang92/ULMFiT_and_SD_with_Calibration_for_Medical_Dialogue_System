{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_glove_base_model.ipynb",
      "provenance": [],
      "mount_file_id": "1FYxqazOGamVBDEjuyJTNcI1a-eI3Sk00",
      "authorship_tag": "ABX9TyM6DZ2DS1EvS++9GCo5YBde",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AoShuang92/ULMFiT_and_SD_with_Calibration_for_Medical_Dialogue_System/blob/main/transformer_glove_base_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIt3FgPPGBdy",
        "outputId": "0c2237c8-dc26-4c9e-d1f1-18373cba824b"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "#system\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from collections import Counter\n",
        "import json\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext import data\n",
        "from nltk.metrics import accuracy, precision, recall, f_measure\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "\n",
        "def seed_everything(seed=27):\n",
        "  #random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YE-i0XZFDFja",
        "outputId": "a25c5401-07d4-4991-8767-63898d6bff54"
      },
      "source": [
        "!pip install \"nltk==3.4.5\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\r\u001b[K     |▎                               | 10kB 24.1MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 31.9MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 22.6MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 21.0MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 21.9MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 16.3MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 16.8MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81kB 17.3MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 15.4MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102kB 16.6MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112kB 16.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122kB 16.6MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 16.6MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143kB 16.6MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153kB 16.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163kB 16.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174kB 16.6MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 16.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 194kB 16.6MB/s eta 0:00:01\r\u001b[K     |████▌                           | 204kB 16.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 215kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 225kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 235kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 245kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 256kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 266kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 276kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 286kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 296kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 307kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 317kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 327kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 337kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 348kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 358kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 368kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 378kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 389kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 399kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 409kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 419kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 430kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 440kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 450kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 460kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 471kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 481kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 491kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 501kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 512kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 522kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 532kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 542kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 552kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 563kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 573kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 583kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 593kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 604kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 614kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 624kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 634kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 645kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 655kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 665kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 675kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 686kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 696kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 706kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 716kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 727kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 737kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 747kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 757kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 768kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 778kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 788kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 798kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 808kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 819kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 829kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 839kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 849kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 860kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 870kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 880kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 890kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 901kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 911kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 921kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 931kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 942kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 952kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 962kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 972kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 983kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 993kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.0MB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.0MB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.0MB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.0MB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0MB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1MB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1MB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1MB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1MB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.1MB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.1MB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2MB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2MB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2MB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.2MB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2MB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2MB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.3MB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.3MB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.3MB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.3MB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.3MB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.3MB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.3MB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.4MB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.4MB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.4MB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.4MB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.4MB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 16.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5) (1.15.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449904 sha256=38de7dc39f6137a926eb41e7c2b8907aad691023aca13056c42f897208d8fc28\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j1aADWfpllS",
        "outputId": "bd5106ee-101b-42c4-dee4-b2637468d1cc"
      },
      "source": [
        "df_train = pd.read_csv(train_dir)\n",
        "df_test= pd.read_csv(test_dir)\n",
        "type(df_test[\"Question\"]),type(df_test[\"Answer\"] ),type(df_train[\"Question\"]),type(df_train[\"Answer\"] )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(pandas.core.series.Series,\n",
              " pandas.core.series.Series,\n",
              " pandas.core.series.Series,\n",
              " pandas.core.series.Series)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_IMdSrfIDc1"
      },
      "source": [
        "max_length = 35\n",
        "train_dir = \"/content/drive/MyDrive/chatbot/combined_qa_train_ID.csv\"\n",
        "test_dir = \"/content/drive/MyDrive/chatbot/combined_qa_test_200_ID.csv\"\n",
        "batch_size = 4\n",
        "\n",
        "\n",
        "def remove_unnecessary(text):\n",
        "    #remove_URL\n",
        "    #url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    #text = url.sub('', text)\n",
        "\n",
        "    #remove_html\n",
        "    #html = re.compile(r'<.*?>')\n",
        "    #text = html.sub('', text)\n",
        "\n",
        "    #remove @\n",
        "    text = re.sub('@[^\\s]+','',text)\n",
        "\n",
        "    #remove_emoji\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    #Removes integers \n",
        "    text = ''.join([i for i in text if not i.isdigit()])         \n",
        "    \n",
        "    #remove_punct\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(table)\n",
        "\n",
        "    #Replaces contractions from a string to their equivalents \n",
        "    contraction_patterns = [(r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), \n",
        "                            (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
        "                            (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'),\n",
        "                            (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), \n",
        "                            (r'dont', 'do not'), (r'wont', 'will not')]\n",
        "    \n",
        "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
        "    for (pattern, repl) in patterns:\n",
        "        text, _= re.subn(pattern, repl, text)\n",
        "\n",
        "    #lemmatize_sentence\n",
        "    sentence_words = text.split(' ')\n",
        "    new_sentence_words = list()\n",
        "    \n",
        "    for sentence_word in sentence_words:\n",
        "        sentence_word = sentence_word.replace('#', '')\n",
        "        new_sentence_word = WordNetLemmatizer().lemmatize(sentence_word.lower(), wordnet.VERB)\n",
        "        new_sentence_words.append(new_sentence_word)\n",
        "        \n",
        "    new_sentence = ' '.join(new_sentence_words)\n",
        "    new_sentence = new_sentence.strip()\n",
        "\n",
        "    return new_sentence.lower()\n",
        "\n",
        "def prepare_csv(train,test):\n",
        "    # idx = np.arange(df_train.shape[0])    \n",
        "    # np.random.shuffle(idx)\n",
        "    # val_size = int(len(idx) * val_ratio)\n",
        "    if not os.path.exists('cache'): # cache is tem memory file \n",
        "        os.makedirs('cache')\n",
        "    \n",
        "    train_temp = train[['Question', 'Answer']].to_csv(\n",
        "        'cache/dataset_train.csv', index=True)\n",
        "    \n",
        "    test_temp = test[['Question', 'Answer']].to_csv(\n",
        "        'cache/dataset_val.csv', index=True) \n",
        "    return  train_temp,  test_temp\n",
        "\n",
        "def get_iterator(dataset, batch_size, train=True,\n",
        "                 shuffle=True, repeat=False, device=None): \n",
        "    dataset_iter = data.Iterator(\n",
        "        dataset, batch_size=batch_size, device=device,\n",
        "        train=train, shuffle=shuffle, repeat=repeat,\n",
        "        sort=False)  \n",
        "    return dataset_iter\n",
        "\n",
        "def get_dataset(fix_length=max_length, lower=False, vectors=None,train_dir = train_dir, test_dir = test_dir, batch_size=batch_size, device=None): \n",
        "    train = pd.read_csv(train_dir,error_bad_lines=False)\n",
        "    test =  pd.read_csv(test_dir,error_bad_lines=False)\n",
        "    train['Question'] = train['Question'].apply(lambda x: remove_unnecessary(x))\n",
        "    train['Answer'] = train['Answer'].apply(lambda x: remove_unnecessary(x))\n",
        "    test['Question'] = test['Question'].apply(lambda x: remove_unnecessary(x))\n",
        "    \n",
        "    test['Answer'] = test['Answer'].apply(lambda x: remove_unnecessary(x))\n",
        "    train_temp,  test_temp = prepare_csv(train,test)\n",
        "    if vectors is not None:\n",
        "        lower=True\n",
        "\n",
        "    TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"spacy\"),init_token='<sos>',eos_token='<eos>',lower=True,batch_first=True, \n",
        "                      fix_length=fix_length)\n",
        "    ID = data.Field(use_vocab=False, sequential=False, dtype=torch.float16)  \n",
        "    train_temps = data.TabularDataset(\n",
        "        path='/content/cache/dataset_train.csv', format='csv', skip_header=True,\n",
        "        fields=[(\"ID\",ID),('Question', TEXT), ('Answer', TEXT)]) \n",
        "    test_temps = data.TabularDataset(\n",
        "        path='/content/cache/dataset_val.csv', format='csv', skip_header=True,\n",
        "        fields=[(\"ID\",ID),('Question', TEXT), ('Answer', TEXT)]) \n",
        "\n",
        "    TEXT.build_vocab(train_temps,test_temps)#, vectors=GloVe(name='6B', dim=300))\n",
        "    ID.build_vocab(train_temps, test_temps)\n",
        "    word_embeddings = TEXT.vocab.vectors\n",
        "    vocab_size = len(TEXT.vocab)\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    print(\"vocab_size_and_ntokens:\",vocab_size,ntokens)\n",
        "    train_loader = get_iterator(train_temps, batch_size=batch_size, \n",
        "                                train=True, shuffle=True,\n",
        "                                repeat=False,device=None)\n",
        "    test_loader = get_iterator(test_temps, batch_size=1, \n",
        "                            train=False, shuffle=False,\n",
        "                            repeat=False, device=None)\n",
        "    print('Train samples:%d'%(len(train_temps)), 'Valid samples:%d'%(len(test_temps)),'Train minibatch nb:%d'%(len(train_loader)),\n",
        "            'Valid minibatch nb:%d'%(len(test_loader)))\n",
        "    return vocab_size, word_embeddings, ntokens, train_loader, test_loader, TEXT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-YbA8eZuRmQ"
      },
      "source": [
        "test =  pd.read_csv(test_dir,error_bad_lines=False)\n",
        "for ans_idx in range(len(test['Answer'])):\n",
        "    print(ans_idx)\n",
        "    remove_unnecessary(test['Answer'][ans_idx])\n",
        "#test['Answer'] = test['Answer'].apply(lambda x: remove_unnecessary(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1gs65-lINWJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6cfa20b-ffe1-4912-f701-ce09c53078e2"
      },
      "source": [
        "\n",
        "vocab_size, word_embeddings, ntokens, train_loader, test_loader, TEXT = get_dataset(fix_length=max_length,train_dir = train_dir, test_dir = test_dir, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size_and_ntokens: 1938 1938\n",
            "Train samples:1001 Valid samples:200 Train minibatch nb:251 Valid minibatch nb:200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCvd3LSEPRW3"
      },
      "source": [
        "def create_masks(question, reply_input):\n",
        "    \n",
        "    def subsequent_mask(size):\n",
        "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "        return mask.unsqueeze(0)\n",
        "    \n",
        "    question_mask = question!=0\n",
        "    question_mask = question_mask.to(device)\n",
        "    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n",
        "     \n",
        "    reply_input_mask = reply_input!=0\n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n",
        "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n",
        "    \n",
        "    return question_mask, reply_input_mask\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(max_length, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len]\n",
        "    return data, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDLfGO6I9stZ"
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements embeddings of the words and adds their positional encodings. \n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, max_len = max_length):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pe = self.create_positinal_encoding(max_len, self.d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def create_positinal_encoding(self, max_len, d_model):\n",
        "        pe = torch.zeros(max_len, d_model).to(device)\n",
        "        for pos in range(max_len):   # for each position of the word\n",
        "            for i in range(0, d_model, 2):   # for each dimension of the each position\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)   # include the batch size\n",
        "        return pe\n",
        "        \n",
        "    def forward(self, encoded_words):\n",
        "        embedding = self.embed(encoded_words) * math.sqrt(self.d_model)\n",
        "        #print(\"embedding\",embedding.size(),encoded_words.size())\n",
        "        #print(\"pe\",self.pe.size())\n",
        "        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n",
        "        embedding = self.dropout(embedding)\n",
        "        return embedding\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, heads, d_model):\n",
        "        \n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % heads == 0\n",
        "        self.d_k = d_model // heads\n",
        "        self.heads = heads\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        self.concat = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        query, key, value of shape: (batch_size, max_len, 512)\n",
        "        mask of shape: (batch_size, 1, 1, max_words)\n",
        "        \"\"\"\n",
        "        # (batch_size, max_len, 512)\n",
        "        query = self.query(query)\n",
        "        key = self.key(key)        \n",
        "        value = self.value(value)   \n",
        "        \n",
        "        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
        "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        \n",
        "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
        "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
        "        #scores = torch.matmul(query, key.permute(2,1,0,0)) / math.sqrt(query.size(-1))\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n",
        "        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n",
        "        weights = self.dropout(weights)\n",
        "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        context = torch.matmul(weights, value)\n",
        "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n",
        "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
        "        # (batch_size, max_len, h * d_k)\n",
        "        interacted = self.concat(context)\n",
        "        return interacted\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, middle_dim = 2048):\n",
        "        super(FeedForward, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
        "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.fc1(x))\n",
        "        out = self.fc2(self.dropout(out))\n",
        "        return out\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, heads):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, embeddings, mask):\n",
        "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
        "        interacted = self.layernorm(interacted + embeddings)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        encoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return encoded\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, heads):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
        "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
        "        query = self.layernorm(query + embeddings)\n",
        "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
        "        interacted = self.layernorm(interacted + query)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        decoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return decoded\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):    \n",
        "    def __init__(self, d_model, heads, num_layers, ntokens):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = ntokens\n",
        "        self.embed = Embeddings(self.vocab_size, d_model)#max_len\n",
        "        self.embed_dec = Embeddings(self.vocab_size, d_model,max_length)\n",
        "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        self.logit = nn.Linear(d_model, self.vocab_size)   \n",
        "        \n",
        "    def encode(self, src_words, src_mask):\n",
        "        src_embeddings = self.embed(src_words)\n",
        "        for layer in self.encoder:\n",
        "            src_embeddings = layer(src_embeddings, src_mask)\n",
        "        return src_embeddings\n",
        "    \n",
        "    def decode(self, target_words, target_mask, src_embeddings, src_mask):\n",
        "        tgt_embeddings = self.embed_dec(target_words)\n",
        "        for layer in self.decoder:\n",
        "            tgt_embeddings = layer(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
        "        return tgt_embeddings\n",
        "        \n",
        "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
        "        encoded = self.encode(src_words, src_mask)\n",
        "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
        "        out = F.log_softmax(self.logit(decoded), dim = 2)\n",
        "        return out\n",
        "\n",
        "class AdamWarmup:\n",
        "    \n",
        "    def __init__(self, model_size, warmup_steps, optimizer):\n",
        "        \n",
        "        self.model_size = model_size\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.optimizer = optimizer\n",
        "        self.current_step = 0\n",
        "        self.lr = 0\n",
        "        \n",
        "    def get_lr(self):\n",
        "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
        "        \n",
        "    def step(self):\n",
        "        # Increment the number of steps each time we call the step function\n",
        "        self.current_step += 1\n",
        "        lr = self.get_lr()\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        # update the learning rate\n",
        "        self.lr = lr\n",
        "        self.optimizer.step()\n",
        "\n",
        "class LossWithLS(nn.Module):\n",
        "\n",
        "    def __init__(self, size, smooth):\n",
        "        super(LossWithLS, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n",
        "        self.confidence = 1.0 - smooth\n",
        "        self.smooth = smooth\n",
        "        self.size = size\n",
        "        \n",
        "    def forward(self, prediction, target, mask):\n",
        "        \"\"\"\n",
        "        prediction of shape: (batch_size, max_words, vocab_size)\n",
        "        target and mask of shape: (batch_size, max_words)\n",
        "        \"\"\"\n",
        "        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n",
        "        target = target.contiguous().view(-1)   # (batch_size * max_words)\n",
        "        mask = mask.float()\n",
        "        mask = mask.view(-1)       # (batch_size * max_words)\n",
        "        labels = prediction.data.clone()\n",
        "        labels.fill_(self.smooth / (self.size - 1))\n",
        "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n",
        "        loss = (loss.sum(1) * mask).sum() / mask.sum()\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw9UJnWeNoto",
        "outputId": "315bf744-228e-4522-a3a8-0d1d1fad67fb"
      },
      "source": [
        "#for i, (question, reply) in enumerate(train_loader): #question=[2,35], reply=[2,37]\n",
        "#TEXT.build_vocab(ff_new)\n",
        "#TEXT.vocab.stoi\n",
        "#TEXT.numericalize([['what']]))\n",
        "#TEXT.vocab.itos\n",
        "#print(TEXT.)\n",
        "def prediction_ids2sentence(pred_ids):\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    #_, next = torch.max(out, dim = 2)\n",
        "    sen_idx = []\n",
        "    for w in pred_ids:\n",
        "        if w == word_map['<eos>']:\n",
        "            break\n",
        "        sen_idx.append(w)\n",
        "    #print(sen_idx)\n",
        "    sentence = ' '.join([rev_word_map[int(sen_idx[k])] for k in range(len(sen_idx))])\n",
        "    return sentence\n",
        "\n",
        "# for i, pair in enumerate(train_loader): \n",
        "#     print(\"pair\", pair.ID[0], pair.Question.shape, pair.Answer.shape)\n",
        "#     print(pair.Question[:,0], prediction_ids2sentence(pair.Question[:,0]))\n",
        "#     print( pair.Answer[:,0], prediction_ids2sentence(pair.Answer[:,0]))\n",
        "#     #TEXT.vocab.stoi\n",
        "#     if i ==0:\n",
        "#         break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pair tensor(144., dtype=torch.float16) torch.Size([4, 35]) torch.Size([4, 35])\n",
            "tensor([2, 2, 2, 2]) <sos> <sos> <sos> <sos>\n",
            "tensor([2, 2, 2, 2]) <sos> <sos> <sos> <sos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViltEEHA96TR"
      },
      "source": [
        "def train(train_loader, transformer, criterion, epoch):    \n",
        "    transformer.train()\n",
        "    sum_loss = 0\n",
        "    count = 0\n",
        "    \n",
        "    for i, pair in enumerate(train_loader): \n",
        "    #for i, (question, reply) in enumerate(train_loader):     \n",
        "        #samples = question.shape[0]\n",
        "        # Move to device\n",
        "        question = pair.Question.to(device)\n",
        "        \n",
        "        reply = pair.Answer.to(device)\n",
        "        \n",
        "        # Prepare Target Data\n",
        "        #print(\"pair\",pair, type(pair),len(pair))\n",
        "        #print(\"reply\",type(reply))\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "\n",
        "        # Create mask and add dimensions\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        print(\"mask type:\", type(question_mask),type(question),question.shape)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        print(\"out\",out.size(), reply_target.size())\n",
        "        reply_target = reply_target.reshape(-1)\n",
        "        print(\"after\",(out.view(-1, ntokens)).size(), reply_target.size())\n",
        "        loss = criterion(out.view(-1, ntokens), reply_target)\n",
        "        #loss = criterion(out, reply_target)\n",
        "        \n",
        "        # Backprop\n",
        "        transformer_optimizer.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        transformer_optimizer.step()\n",
        "        \n",
        "        #sum_loss += loss.item() * samples\n",
        "        #count += samples\n",
        "        #break\n",
        "        # if i % 100 == 0:\n",
        "        #     print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))\n",
        "\n",
        "def valid (test_loader,transformer): \n",
        "    all_blue = []\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    #rev_word_map = {v: k for k, v in word_embeddings.items()}\n",
        "    transformer.eval()\n",
        "    for i, pair in enumerate(test_loader):\n",
        "    #for i, (question, reply) in enumerate(test_loader):\n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        #loss = criterion(out, reply_target, reply_target_mask)\n",
        "        _, next = torch.max(out, dim = 2)# 2x51\n",
        "        for idx in range(next.shape[0]):\n",
        "            pred_sentence= prediction_ids2sentence(next[idx]).split()\n",
        "            gt=prediction_ids2sentence(reply_target[idx]).split()\n",
        "            BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "            # BLEU_2 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 0, 0))\n",
        "            # BLEU_3 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 0))\n",
        "            # BLEU_4 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 1))\n",
        "            all_blue.append(BLEU_1)\n",
        "    #print(\"BLEU_score:\",np.mean(all_blue))\n",
        "    return np.mean(all_blue)\n",
        "\n",
        "def evaluate(transformer, question, question_mask, max_len):\n",
        "    \"\"\"\n",
        "    Performs Greedy Decoding with a batch size of 1\n",
        "    \"\"\"\n",
        "    #rev_word_map = {v: k for k, v in word_embeddings.items()}\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    transformer.eval()\n",
        "    start_token = word_map['<sos>']\n",
        "    encoded = transformer.encode(question, question_mask)\n",
        "    words = torch.LongTensor([[start_token]]).to(device)\n",
        "    next_word = -22\n",
        "    while next_word != word_map['<eos>']:\n",
        "    #for step in range(max_len - 1):\n",
        "        size = words.shape[1]\n",
        "        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
        "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
        "        predictions = transformer.logit(decoded[:, -1])\n",
        "        _, next_word = torch.max(predictions, dim = 1)\n",
        "        next_word = next_word.item()\n",
        "        if next_word == word_map['<eos>'] or words.shape[1]==(max_len+1):\n",
        "            break\n",
        "        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n",
        "        \n",
        "    # Construct Sentence\n",
        "    if words.dim() == 2:\n",
        "        words = words.squeeze(0)\n",
        "        words = words.tolist()\n",
        "        \n",
        "    sen_idx = [w for w in words if w not in {word_map['<sos>']}]\n",
        "    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n",
        "    \n",
        "\n",
        "    return sentence\n",
        "\n",
        "def prediction_ids2sentence(pred_ids):\n",
        "    #rev_word_map = {v: k for k, v in word_embeddings.items()}\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    #_, next = torch.max(out, dim = 2)\n",
        "    sen_idx = []\n",
        "    for w in pred_ids:\n",
        "        if w == word_map['<eos>']:\n",
        "            break\n",
        "        sen_idx.append(w)\n",
        "    #print(sen_idx)\n",
        "    sentence = ' '.join([rev_word_map[int(sen_idx[k])] for k in range(len(sen_idx))])\n",
        "    return sentence\n",
        "\n",
        "from nltk.metrics import accuracy, precision, recall, f_measure\n",
        "\n",
        "# def evaluate_matrics(transformer,test_loader):\n",
        "#     all_blue = []\n",
        "#     all_acc = []\n",
        "#     all_prec = []\n",
        "#     all_rec = []\n",
        "#     all_f_score = []\n",
        "#     word_map = TEXT.vocab.stoi\n",
        "#     rev_word_map = TEXT.vocab.itos\n",
        "#     #rev_word_map = {v: k for k, v in word_map_all.items()}\n",
        "#     transformer.eval()\n",
        "#     for i, pair in enumerate(test_loader):\n",
        "#     #for i, (question, reply) in enumerate(test_loader):\n",
        "#         question = pair.Question.to(device)\n",
        "#         reply = pair.Answer.to(device)\n",
        "        \n",
        "#         reply_input = reply[:, :-1]\n",
        "#         reply_target = reply[:, 1:]\n",
        "#         question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "#         out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "#         #loss = criterion(out, reply_target, reply_target_mask)\n",
        "#         _, next = torch.max(out, dim = 2)# 2x51\n",
        "#         #print(\"next\",next.size(),\"next0\",next[0].size(),\"next1\",next[1].size())\n",
        "#         for idx in range(next.shape[0]):\n",
        "#             pred_sentence= prediction_ids2sentence(next[idx]).split()\n",
        "            \n",
        "#             gt=prediction_ids2sentence(reply_target[idx]).split()\n",
        "#             BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "#             reference_set = set(gt)\n",
        "#             test_set = set(pred_sentence)\n",
        "#             prec = precision(reference_set, test_set)\n",
        "#             rec = recall(reference_set, test_set)\n",
        "#             f_score = f_measure(reference_set, test_set)\n",
        "#             all_blue.append(BLEU_1)\n",
        "#             all_prec.append(prec)\n",
        "#             all_rec.append(rec)\n",
        "#             all_f_score.append(f_score)\n",
        "#     print(\"BLEU_SCORE:\",np.mean(all_blue), \"Precision:\",np.mean(all_prec), \"F1_Score:\",np.mean(all_f_score))\n",
        "\n",
        "def evaluate_matrics(transformer,test_loader):\n",
        "    sum_loss = 0\n",
        "    all_blue1 = []\n",
        "    all_blue2 = []\n",
        "    all_blue3 = []\n",
        "    all_blue4 = []\n",
        "\n",
        "    all_acc = []\n",
        "    all_prec = []\n",
        "    all_rec = []\n",
        "    all_f_score = []\n",
        "    all_meteor = []\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    #rev_word_map = {v: k for k, v in word_map_all.items()}\n",
        "    transformer.eval()\n",
        "    for i, pair in enumerate(test_loader):\n",
        "        #print(i)\n",
        "    \n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        \n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        reply_target_mask = reply_target.reshape(-1)\n",
        "        loss = criterion(out.view(-1, ntokens), reply_target_mask)\n",
        "        #loss = criterion(out, reply_target, reply_target_mask)\n",
        "        sum_loss += loss.item()\n",
        "        #loss = criterion(out, reply_target, reply_target_mask)\n",
        "        _, next = torch.max(out, dim = 2)# 2x51\n",
        "        #print(\"next\",next.size(),\"next0\",next[0].size(),\"next1\",next[1].size())\n",
        "        for idx in range(next.shape[0]):\n",
        "            pred_sentence= prediction_ids2sentence(next[idx]).split()\n",
        "            \n",
        "            gt=prediction_ids2sentence(reply_target[idx]).split()\n",
        "            BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "            if BLEU_1 >= 0.94:\n",
        "                print(idx, BLEU_1,pred_sentence,gt)\n",
        "            #BLEU_2 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 0, 0))\n",
        "            #BLEU_3 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 0))\n",
        "            #BLEU_4 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 1))\n",
        "            #print (type(gt),type(pred_sentence))\n",
        "            reference_set = set(gt)\n",
        "            test_set = set(pred_sentence)\n",
        "            prec = precision(reference_set, test_set)\n",
        "            rec = recall(reference_set, test_set)\n",
        "            f_score = f_measure(reference_set, test_set)\n",
        "            meteor = single_meteor_score( str(gt), str(pred_sentence))\n",
        "            all_blue1.append(BLEU_1)\n",
        "            \n",
        "            #all_blue2.append(BLEU_2)\n",
        "            #all_blue3.append(BLEU_3)\n",
        "            #all_blue4.append(BLEU_4)\n",
        "            all_prec.append(prec)\n",
        "            all_rec.append(rec)\n",
        "            all_f_score.append(f_score)\n",
        "            all_meteor.append(meteor)\n",
        "            #\"Recall:\",np.mean(all_rec),\n",
        "    pre = np.mean(all_prec)\n",
        "    recall_score = np.mean(all_rec)\n",
        "    f1 = np.mean(all_f_score)\n",
        "    met = np.mean(all_meteor)\n",
        "    ppl = math.exp(sum_loss/i)\n",
        "\n",
        "    print(\"BLEU_SCORE1:\",np.mean(all_blue1), \"Precision:\",np.mean(all_prec), \"Recall:\",np.mean(all_rec),\"F1_Score:\",np.mean(all_f_score), \"Meteor:\",np.mean(all_meteor),\"PPL:\",math.exp(sum_loss/i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SUePCc1CUoM"
      },
      "source": [
        "class _ECELoss(nn.Module):\n",
        "    def __init__(self, n_bins=15):\n",
        "        \"\"\"\n",
        "        n_bins (int): number of confidence interval bins\n",
        "        \"\"\"\n",
        "        super(_ECELoss, self).__init__()\n",
        "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "        self.bin_lowers = bin_boundaries[:-1]\n",
        "        self.bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        softmaxes = F.softmax(logits, dim=1)\n",
        "        confidences, predictions = torch.max(softmaxes, 1)\n",
        "        accuracies = predictions.eq(labels)\n",
        "        ece = torch.zeros(1, device=logits.device)\n",
        "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
        "            # Calculated |confidence - accuracy| in each bin\n",
        "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
        "            prop_in_bin = in_bin.float().mean()\n",
        "            if prop_in_bin.item() > 0:\n",
        "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "        return ece\n",
        "\n",
        "def evaluation(model, test_loader):\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    logits_list = []\n",
        "    labels_list = []\n",
        "    all_blue = []\n",
        "    all_acc = []\n",
        "    all_rec = []\n",
        "    all_f_score = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        #for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        for i, pair in enumerate(test_loader):\n",
        "    \n",
        "            inputs = pair.Question\n",
        "            targets = pair.Answer\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            targets = targets[:, 1:]\n",
        "            input_mask, targets_mask = create_masks(inputs, targets)\n",
        "            outputs = model(inputs, input_mask, targets, targets_mask)\n",
        "            #print(outputs.shape)\n",
        "            logits_list.append(outputs)\n",
        "            labels_list.append(targets)\n",
        "            # target_loss = targets.reshape(-1)\n",
        "            # loss = criterion(outputs.view(-1, ntokens), target_loss)\n",
        "            # test_loss += loss.item()\n",
        "            _, predicted = outputs.max(2)\n",
        "        \n",
        "            for idx in range(predicted.shape[0]):\n",
        "                \n",
        "                pred_sentence= prediction_ids2sentence(predicted[idx]).split()\n",
        "                gt=prediction_ids2sentence(targets[idx]).split()\n",
        "                BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "                reference_set = set(gt)\n",
        "                test_set = set(pred_sentence)\n",
        "                #rec = recall(reference_set, test_set)\n",
        "                #f_score = f_measure(reference_set, test_set)\n",
        "                all_blue.append(BLEU_1)\n",
        "                #all_rec.append(rec)\n",
        "                #all_f_score.append(f_score)\n",
        "\n",
        "    logits_all = torch.cat(logits_list).cuda()\n",
        "    labels_all = torch.cat(labels_list).cuda()\n",
        "    \n",
        "    return np.mean(all_blue),logits_all, labels_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K5L4NMbxe9l5",
        "outputId": "8ce6f2bc-1d1f-4f0e-8829-c21453ca3f87"
      },
      "source": [
        "seed_everything()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens)\n",
        "model = model.to(device)\n",
        "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "\n",
        "best_blue = 0\n",
        "best_epoch = 0\n",
        "for epoch in range(50):\n",
        "    train(train_loader, model, criterion, epoch)\n",
        "    blue_score = valid (test_loader,model)\n",
        "    if blue_score > best_blue:\n",
        "        best_blue = blue_score\n",
        "        best_epoch = epoch\n",
        "        # state = {'epoch': epoch, 'transformer': model, 'transformer_optimizer': transformer_optimizer}\n",
        "        # torch.save(state, '/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_model_object.pth.tar')\n",
        "        # torch.save(model.state_dict(), '/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_models_state_dict.pth')\n",
        "    print('cur epoch:%d, cur blue:%.5f, best epoch:%d, best blue:%.5f'%(epoch,blue_score, best_epoch, best_blue))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n",
            "mask type: <class 'torch.Tensor'> <class 'torch.Tensor'> torch.Size([4, 35])\n",
            "out torch.Size([4, 34, 1938]) torch.Size([4, 34])\n",
            "after torch.Size([136, 1938]) torch.Size([136])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-dedbeedafaca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mbest_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mblue_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mblue_score\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_blue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-8b9a39c39880>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, transformer, criterion, epoch)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtransformer_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mtransformer_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#sum_loss += loss.item() * samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-26c4f3db3b9a>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLossWithLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T40WZ4bI-Im2",
        "outputId": "b44defd2-4d9c-4d18-dc39-01071311504d"
      },
      "source": [
        "seed_everything()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens)\n",
        "model = model.to(device)\n",
        "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "\n",
        "best_blue = 0\n",
        "best_epoch = 0\n",
        "for epoch in range(50):\n",
        "    train(train_loader, model, criterion, epoch)\n",
        "    blue_score = valid (test_loader,model)\n",
        "    if blue_score > best_blue:\n",
        "        best_blue = blue_score\n",
        "        best_epoch = epoch\n",
        "        state = {'epoch': epoch, 'transformer': model, 'transformer_optimizer': transformer_optimizer}\n",
        "        torch.save(state, '/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_model_object.pth.tar')\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_models_state_dict.pth')\n",
        "    print('cur epoch:%d, cur blue:%.5f, best epoch:%d, best blue:%.5f'%(epoch,blue_score, best_epoch, best_blue))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cur epoch:0, cur blue:0.00369, best epoch:0, best blue:0.00369\n",
            "cur epoch:1, cur blue:0.11305, best epoch:1, best blue:0.11305\n",
            "cur epoch:2, cur blue:0.15489, best epoch:2, best blue:0.15489\n",
            "cur epoch:3, cur blue:0.17268, best epoch:3, best blue:0.17268\n",
            "cur epoch:4, cur blue:0.21838, best epoch:4, best blue:0.21838\n",
            "cur epoch:5, cur blue:0.25695, best epoch:5, best blue:0.25695\n",
            "cur epoch:6, cur blue:0.27231, best epoch:6, best blue:0.27231\n",
            "cur epoch:7, cur blue:0.31692, best epoch:7, best blue:0.31692\n",
            "cur epoch:8, cur blue:0.33635, best epoch:8, best blue:0.33635\n",
            "cur epoch:9, cur blue:0.32424, best epoch:8, best blue:0.33635\n",
            "cur epoch:10, cur blue:0.35562, best epoch:10, best blue:0.35562\n",
            "cur epoch:11, cur blue:0.35893, best epoch:11, best blue:0.35893\n",
            "cur epoch:12, cur blue:0.31590, best epoch:11, best blue:0.35893\n",
            "cur epoch:13, cur blue:0.34171, best epoch:11, best blue:0.35893\n",
            "cur epoch:14, cur blue:0.33684, best epoch:11, best blue:0.35893\n",
            "cur epoch:15, cur blue:0.30879, best epoch:11, best blue:0.35893\n",
            "cur epoch:16, cur blue:0.32222, best epoch:11, best blue:0.35893\n",
            "cur epoch:17, cur blue:0.35586, best epoch:11, best blue:0.35893\n",
            "cur epoch:18, cur blue:0.34513, best epoch:11, best blue:0.35893\n",
            "cur epoch:19, cur blue:0.35853, best epoch:11, best blue:0.35893\n",
            "cur epoch:20, cur blue:0.38074, best epoch:20, best blue:0.38074\n",
            "cur epoch:21, cur blue:0.36107, best epoch:20, best blue:0.38074\n",
            "cur epoch:22, cur blue:0.38894, best epoch:22, best blue:0.38894\n",
            "cur epoch:23, cur blue:0.39906, best epoch:23, best blue:0.39906\n",
            "cur epoch:24, cur blue:0.38361, best epoch:23, best blue:0.39906\n",
            "cur epoch:25, cur blue:0.38925, best epoch:23, best blue:0.39906\n",
            "cur epoch:26, cur blue:0.39091, best epoch:23, best blue:0.39906\n",
            "cur epoch:27, cur blue:0.39368, best epoch:23, best blue:0.39906\n",
            "cur epoch:28, cur blue:0.40098, best epoch:28, best blue:0.40098\n",
            "cur epoch:29, cur blue:0.40530, best epoch:29, best blue:0.40530\n",
            "cur epoch:30, cur blue:0.39553, best epoch:29, best blue:0.40530\n",
            "cur epoch:31, cur blue:0.39377, best epoch:29, best blue:0.40530\n",
            "cur epoch:32, cur blue:0.40422, best epoch:29, best blue:0.40530\n",
            "cur epoch:33, cur blue:0.40328, best epoch:29, best blue:0.40530\n",
            "cur epoch:34, cur blue:0.39799, best epoch:29, best blue:0.40530\n",
            "cur epoch:35, cur blue:0.40004, best epoch:29, best blue:0.40530\n",
            "cur epoch:36, cur blue:0.42195, best epoch:36, best blue:0.42195\n",
            "cur epoch:37, cur blue:0.42280, best epoch:37, best blue:0.42280\n",
            "cur epoch:38, cur blue:0.40664, best epoch:37, best blue:0.42280\n",
            "cur epoch:39, cur blue:0.40779, best epoch:37, best blue:0.42280\n",
            "cur epoch:40, cur blue:0.41829, best epoch:37, best blue:0.42280\n",
            "cur epoch:41, cur blue:0.40756, best epoch:37, best blue:0.42280\n",
            "cur epoch:42, cur blue:0.41085, best epoch:37, best blue:0.42280\n",
            "cur epoch:43, cur blue:0.42920, best epoch:43, best blue:0.42920\n",
            "cur epoch:44, cur blue:0.40741, best epoch:43, best blue:0.42920\n",
            "cur epoch:45, cur blue:0.42100, best epoch:43, best blue:0.42920\n",
            "cur epoch:46, cur blue:0.41699, best epoch:43, best blue:0.42920\n",
            "cur epoch:47, cur blue:0.42170, best epoch:43, best blue:0.42920\n",
            "cur epoch:48, cur blue:0.40786, best epoch:43, best blue:0.42920\n",
            "cur epoch:49, cur blue:0.42119, best epoch:43, best blue:0.42920\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88gL4dPLA5-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09c0754c-ecf3-4648-c986-e04ff10858c8"
      },
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_models_state_dict.pth'))\n",
        "ece_criterion = _ECELoss().to(device)\n",
        "bleu, logits_all, labels_all = evaluation(model, test_loader)\n",
        "logits_all = logits_all.view(-1,ntokens)\n",
        "labels_all = labels_all.view(-1)\n",
        "temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
        "\n",
        "evaluate_matrics(model,test_loader)\n",
        "temperature_ece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 0.9411764705882353 ['try', 'up', 'and', 'move', 'about', 'gently', 'for', 'a', 'short', 'period', 'every', 'hour', 'would', 'help', 'relieve', 'muscle', 'stiffness'] ['stand', 'up', 'and', 'move', 'about', 'gently', 'for', 'a', 'short', 'period', 'every', 'hour', 'would', 'help', 'relieve', 'muscle', 'stiffness']\n",
            "0 0.9523809523809523 ['the', 'most', 'back', 'problems', 'start', 'for', 'no', 'obvious', 'reason', 'back', 'pain', 'can', 'be', 'cause', 'by', 'stay', 'in', 'one', 'position', 'too', 'long'] ['although', 'most', 'back', 'problems', 'start', 'for', 'no', 'obvious', 'reason', 'back', 'pain', 'can', 'be', 'cause', 'by', 'stay', 'in', 'one', 'position', 'too', 'long']\n",
            "0 0.9583333333333334 ['try', 'exercise', 'a', 'painful', 'inflame', 'or', 'hot', 'joint', 'insteadgently', 'move', 'the', 'joint', 'through', 'its', 'range', 'of', 'movement', 'to', 'help', 'reduce', 'stiffness', 'and', 'improve', 'circulation'] ['avoid', 'exercise', 'a', 'painful', 'inflame', 'or', 'hot', 'joint', 'insteadgently', 'move', 'the', 'joint', 'through', 'its', 'range', 'of', 'movement', 'to', 'help', 'reduce', 'stiffness', 'and', 'improve', 'circulation']\n",
            "0 0.9523809523809523 ['vitamin', 'attention', 'to', 'good', 'technique', 'and', 'try', 'to', 'move', 'smoothly', 'do', 'n’t', 'force', 'a', 'joint', 'beyond', 'a', 'comfortable', 'range', 'of', 'movement'] ['pay', 'attention', 'to', 'good', 'technique', 'and', 'try', 'to', 'move', 'smoothly', 'do', 'n’t', 'force', 'a', 'joint', 'beyond', 'a', 'comfortable', 'range', 'of', 'movement']\n",
            "0 1.0 ['the', 'exercise', 'you', 'choose', 'should', 'be', 'something', 'you', 'enjoy', 'and', 'you', '’re', 'commit', 'to', 'do'] ['the', 'exercise', 'you', 'choose', 'should', 'be', 'something', 'you', 'enjoy', 'and', 'you', '’re', 'commit', 'to', 'do']\n",
            "0 0.9411764705882353 ['the', 'recommend', 'you', 'try', 'include', 'monounsaturated', 'and', 'polyunsaturated', 'fat', 'in', 'your', 'diet', 'which', 'be', 'heart', 'healthy', 'fat'] ['we', 'recommend', 'you', 'try', 'include', 'monounsaturated', 'and', 'polyunsaturated', 'fat', 'in', 'your', 'diet', 'which', 'be', 'heart', 'healthy', 'fat']\n",
            "BLEU_SCORE1: 0.4292045823831515 Precision: 0.5183647308700782 Recall: 0.48083364084277447 F1_Score: 0.49380273548549075 Meteor: 0.407960550577293 PPL: 7.743106942968297\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3702329993247986"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7RMxCMaHGez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f9e6276-b109-48e2-aee3-94e3284b5b7b"
      },
      "source": [
        "seed_everything()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens)\n",
        "model = model.to(device)\n",
        "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_models_state_dict.pth'))\n",
        "ece_criterion = _ECELoss().to(device)\n",
        "bleu, logits_all, labels_all = evaluation(model, test_loader)\n",
        "logits_all = logits_all.view(-1,ntokens)\n",
        "labels_all = labels_all.view(-1)\n",
        "temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
        "\n",
        "evaluate_matrics(model,test_loader)\n",
        "temperature_ece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 0.9411764705882353 ['try', 'up', 'and', 'move', 'about', 'gently', 'for', 'a', 'short', 'period', 'every', 'hour', 'would', 'help', 'relieve', 'muscle', 'stiffness'] ['stand', 'up', 'and', 'move', 'about', 'gently', 'for', 'a', 'short', 'period', 'every', 'hour', 'would', 'help', 'relieve', 'muscle', 'stiffness']\n",
            "0 0.9523809523809523 ['the', 'most', 'back', 'problems', 'start', 'for', 'no', 'obvious', 'reason', 'back', 'pain', 'can', 'be', 'cause', 'by', 'stay', 'in', 'one', 'position', 'too', 'long'] ['although', 'most', 'back', 'problems', 'start', 'for', 'no', 'obvious', 'reason', 'back', 'pain', 'can', 'be', 'cause', 'by', 'stay', 'in', 'one', 'position', 'too', 'long']\n",
            "0 0.9583333333333334 ['try', 'exercise', 'a', 'painful', 'inflame', 'or', 'hot', 'joint', 'insteadgently', 'move', 'the', 'joint', 'through', 'its', 'range', 'of', 'movement', 'to', 'help', 'reduce', 'stiffness', 'and', 'improve', 'circulation'] ['avoid', 'exercise', 'a', 'painful', 'inflame', 'or', 'hot', 'joint', 'insteadgently', 'move', 'the', 'joint', 'through', 'its', 'range', 'of', 'movement', 'to', 'help', 'reduce', 'stiffness', 'and', 'improve', 'circulation']\n",
            "0 0.9523809523809523 ['vitamin', 'attention', 'to', 'good', 'technique', 'and', 'try', 'to', 'move', 'smoothly', 'do', 'n’t', 'force', 'a', 'joint', 'beyond', 'a', 'comfortable', 'range', 'of', 'movement'] ['pay', 'attention', 'to', 'good', 'technique', 'and', 'try', 'to', 'move', 'smoothly', 'do', 'n’t', 'force', 'a', 'joint', 'beyond', 'a', 'comfortable', 'range', 'of', 'movement']\n",
            "0 1.0 ['the', 'exercise', 'you', 'choose', 'should', 'be', 'something', 'you', 'enjoy', 'and', 'you', '’re', 'commit', 'to', 'do'] ['the', 'exercise', 'you', 'choose', 'should', 'be', 'something', 'you', 'enjoy', 'and', 'you', '’re', 'commit', 'to', 'do']\n",
            "0 0.9411764705882353 ['the', 'recommend', 'you', 'try', 'include', 'monounsaturated', 'and', 'polyunsaturated', 'fat', 'in', 'your', 'diet', 'which', 'be', 'heart', 'healthy', 'fat'] ['we', 'recommend', 'you', 'try', 'include', 'monounsaturated', 'and', 'polyunsaturated', 'fat', 'in', 'your', 'diet', 'which', 'be', 'heart', 'healthy', 'fat']\n",
            "BLEU_SCORE1: 0.4292045823831515 Precision: 0.5183647308700782 Recall: 0.48083364084277447 F1_Score: 0.49380273548549075 Meteor: 0.407960550577293 PPL: 7.743106942968297\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3702329993247986"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGmFaNzsn0mS",
        "outputId": "24417ee5-5200-4ec0-983f-08dcacf4ecec"
      },
      "source": [
        "seed_everything()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_models_state_dict_SSL.pth'))\n",
        "ece_criterion = _ECELoss().to(device)\n",
        "bleu, logits_all, labels_all = evaluation(model, test_loader)\n",
        "logits_all = logits_all.view(-1,ntokens)\n",
        "labels_all = labels_all.view(-1)\n",
        "temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
        "\n",
        "evaluate_matrics(model,test_loader)\n",
        "temperature_ece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 0.9411764705882353 ['try', 'up', 'and', 'move', 'about', 'gently', 'for', 'a', 'short', 'period', 'every', 'hour', 'would', 'help', 'relieve', 'muscle', 'stiffness'] ['stand', 'up', 'and', 'move', 'about', 'gently', 'for', 'a', 'short', 'period', 'every', 'hour', 'would', 'help', 'relieve', 'muscle', 'stiffness']\n",
            "0 0.9523809523809523 ['back', 'most', 'back', 'problems', 'start', 'for', 'no', 'obvious', 'reason', 'back', 'pain', 'can', 'be', 'cause', 'by', 'stay', 'in', 'one', 'position', 'too', 'long'] ['although', 'most', 'back', 'problems', 'start', 'for', 'no', 'obvious', 'reason', 'back', 'pain', 'can', 'be', 'cause', 'by', 'stay', 'in', 'one', 'position', 'too', 'long']\n",
            "0 0.9333333333333333 ['back', 'strainous', 'sport', 'or', 'heavy', 'lift', 'until', 'you', 'have', 'less', 'discomfort', 'and', 'regain', 'good', 'movement'] ['avoid', 'strainous', 'sport', 'or', 'heavy', 'lift', 'until', 'you', 'have', 'less', 'discomfort', 'and', 'regain', 'good', 'movement']\n",
            "0 0.9523809523809523 ['vitamin', 'attention', 'to', 'good', 'technique', 'and', 'try', 'to', 'move', 'smoothly', 'do', 'n’t', 'force', 'a', 'joint', 'beyond', 'a', 'comfortable', 'range', 'of', 'movement'] ['pay', 'attention', 'to', 'good', 'technique', 'and', 'try', 'to', 'move', 'smoothly', 'do', 'n’t', 'force', 'a', 'joint', 'beyond', 'a', 'comfortable', 'range', 'of', 'movement']\n",
            "0 0.9310344827586207 ['vitamin', 'exercise', 'to', 'do', 'too', 'much', 'or', 'push', 'too', 'hard', 'too', 'soon', 'if', 'you', '’re', 'short', 'of', 'breath', 'or', 'in', 'pain', 'ease', 'back', 'on', 'the', 'intensity', 'of', 'your', 'exercise'] ['avoid', 'try', 'to', 'do', 'too', 'much', 'or', 'push', 'too', 'hard', 'too', 'soon', 'if', 'you', '’re', 'short', 'of', 'breath', 'or', 'in', 'pain', 'ease', 'back', 'on', 'the', 'intensity', 'of', 'your', 'exercise']\n",
            "0 0.9333333333333333 ['try', 'incidental', 'activity', 'in', 'your', 'lifestyle', 'for', 'example', 'walk', 'to', 'nearby', 'shop', 'instead', 'of', 'drive'] ['increase', 'incidental', 'activity', 'in', 'your', 'lifestyle', 'for', 'example', 'walk', 'to', 'nearby', 'shop', 'instead', 'of', 'drive']\n",
            "0 0.9375 ['try', 'be', 'important', 'to', 'find', 'the', 'right', 'balance', 'of', 'rest', 'and', 'exercise', 'if', 'you', '’re', 'not', 'sure', 'what', 'the', 'right', 'balance', 'be', 'for', 'you', 'talk', 'with', 'the', 'medical', 'professional', 'for', 'some', 'advice'] ['it', '’s', 'important', 'to', 'find', 'the', 'right', 'balance', 'of', 'rest', 'and', 'exercise', 'if', 'you', '’re', 'not', 'sure', 'what', 'the', 'right', 'balance', 'be', 'for', 'you', 'talk', 'with', 'the', 'medical', 'professional', 'for', 'some', 'advice']\n",
            "0 0.9333333333333333 ['try', 'contain', 'high', 'level', 'of', 'sugar', 'and', 'it', 'be', 'recommend', 'they', 'be', 'eat', 'in', 'moderation'] ['bananas', 'contain', 'high', 'level', 'of', 'sugar', 'and', 'it', 'be', 'recommend', 'they', 'be', 'eat', 'in', 'moderation']\n",
            "0 0.92 ['vitamin', 'of', 'contain', 'more', 'sugar', 'than', 'some', 'other', 'fruit', 'bananas', 'and', 'mangos', 'be', 'a', 'few', 'fruit', 'that', 'contain', 'more', 'sugar', 'in', 'them', 'than', 'other', 'fruit'] ['some', 'fruit', 'contain', 'more', 'sugar', 'than', 'some', 'other', 'fruit', 'bananas', 'and', 'mangos', 'be', 'a', 'few', 'fruit', 'that', 'contain', 'more', 'sugar', 'in', 'them', 'than', 'other', 'fruit']\n",
            "0 0.9545454545454546 ['try', 'a', 'mediumsize', 'mango', 'and', 'half', 'a', 'banana', 'have', 'the', 'same', 'calories', 'as', 'a', 'tennisball', 'size', 'apple', 'opt', 'for', 'an', 'apple', 'instead'] ['half', 'a', 'mediumsize', 'mango', 'and', 'half', 'a', 'banana', 'have', 'the', 'same', 'calories', 'as', 'a', 'tennisball', 'size', 'apple', 'opt', 'for', 'an', 'apple', 'instead']\n",
            "0 0.9285714285714286 ['try', 'include', 'calcium', 'citrate', 'or', 'calcium', 'carbonate', 'supplement', 'or', 'try', 'other', 'source', 'of', 'calcium'] ['try', 'take', 'calcium', 'citrate', 'or', 'calcium', 'carbonate', 'supplement', 'or', 'try', 'other', 'source', 'of', 'calcium']\n",
            "0 0.95 ['try', 'include', 'at', 'least', '–', 'minutes', 'of', 'exposure', 'to', 'the', 'sun', 'every', 'day', 'to', 'increase', 'vitamin', 'd', 'in', 'the', 'body'] ['try', 'have', 'at', 'least', '–', 'minutes', 'of', 'exposure', 'to', 'the', 'sun', 'every', 'day', 'to', 'increase', 'vitamin', 'd', 'in', 'the', 'body']\n",
            "0 0.9393939393939394 ['vitamin', 'recommend', 'you', 'discuss', 'with', 'your', 'nutritionist', 'to', 'know', 'exactly', 'what', 'percentage', 'of', 'your', 'total', 'calorie', 'intake', 'should', 'come', 'from', 'fat', 'as', 'per', 'your', 'need', 'take', 'into', 'account', 'your', 'medical', 'history', 'and', 'fitness'] ['we', 'advise', 'you', 'discuss', 'with', 'your', 'nutritionist', 'to', 'know', 'exactly', 'what', 'percentage', 'of', 'your', 'total', 'calorie', 'intake', 'should', 'come', 'from', 'fat', 'as', 'per', 'your', 'need', 'take', 'into', 'account', 'your', 'medical', 'history', 'and', 'fitness']\n",
            "0 0.9411764705882353 ['vitamin', 'recommend', 'you', 'try', 'include', 'monounsaturated', 'and', 'polyunsaturated', 'fat', 'in', 'your', 'diet', 'which', 'be', 'heart', 'healthy', 'fat'] ['we', 'recommend', 'you', 'try', 'include', 'monounsaturated', 'and', 'polyunsaturated', 'fat', 'in', 'your', 'diet', 'which', 'be', 'heart', 'healthy', 'fat']\n",
            "0 0.9354838709677419 ['vitamin', 'suggest', 'that', 'adults', 'a', 'combination', 'of', 'carbohydrates', 'such', 'as', 'fruit', 'juice', 'and', 'a', 'protein', 'such', 'as', 'milk', 'within', 'minutes', 'of', 'your', 'workout', 'this', 'allow', 'your', 'body', 'to', 'best', 'utilise', 'protein'] ['research', 'suggest', 'that', 'consume', 'a', 'combination', 'of', 'carbohydrates', 'such', 'as', 'fruit', 'juice', 'and', 'a', 'protein', 'such', 'as', 'milk', 'within', 'minutes', 'of', 'your', 'workout', 'this', 'allow', 'your', 'body', 'to', 'best', 'utilise', 'protein']\n",
            "BLEU_SCORE1: 0.432117198665572 Precision: 0.5013206707205613 Recall: 0.47854662319421903 F1_Score: 0.48636758259571017 Meteor: 0.4218850844127235 PPL: 7.810682039402031\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.37642902135849"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-a7cbX2ds8L5",
        "outputId": "e0ec53ac-0ba4-41c6-e609-d98c7ec31d91"
      },
      "source": [
        "seed_everything()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_models_state_dict_SSL_LS1.pth'))\n",
        "ece_criterion = _ECELoss().to(device)\n",
        "bleu, logits_all, labels_all = evaluation(model, test_loader)\n",
        "logits_all = logits_all.view(-1,ntokens)\n",
        "labels_all = labels_all.view(-1)\n",
        "temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
        "\n",
        "evaluate_matrics(model,test_loader)\n",
        "temperature_ece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 0.9411764705882353 ['a', 'hygiene', 'measure', 'such', 'as', 'a', 'warm', 'bath', 'before', 'bed', 'can', 'help', 'you', 'prepare', 'sleep', 'for', 'sleep'] ['sleep', 'hygiene', 'measure', 'such', 'as', 'a', 'warm', 'bath', 'before', 'bed', 'can', 'help', 'you', 'better', 'prepare', 'for', 'sleep']\n",
            "0 0.9411764705882353 ['try', 'up', 'and', 'move', 'about', 'gently', 'for', 'a', 'short', 'period', 'every', 'hour', 'would', 'help', 'relieve', 'muscle', 'stiffness'] ['stand', 'up', 'and', 'move', 'about', 'gently', 'for', 'a', 'short', 'period', 'every', 'hour', 'would', 'help', 'relieve', 'muscle', 'stiffness']\n",
            "0 1.0 ['the', 'assessment', 'will', 'help', 'decide', 'if', 'the', 'potential', 'benefit', 'of', 'exercise', 'outweigh', 'the', 'risk', 'for', 'you'] ['the', 'assessment', 'will', 'help', 'decide', 'if', 'the', 'potential', 'benefit', 'of', 'exercise', 'outweigh', 'the', 'risk', 'for', 'you']\n",
            "0 0.9473684210526315 ['if', 'your', '’ve', 'have', 'a', 'joint', 'replace', 'discuss', 'with', 'the', 'health', 'professional', 'which', 'movements', 'you', 'should', 'limit', 'or', 'avoid'] ['if', 'you', '’ve', 'have', 'a', 'joint', 'replace', 'discuss', 'with', 'the', 'health', 'professional', 'which', 'movements', 'you', 'should', 'limit', 'or', 'avoid']\n",
            "0 0.9583333333333334 ['avoid', 'exercise', 'a', 'painful', 'inflame', 'or', 'hot', 'joint', 'rather', 'move', 'the', 'joint', 'through', 'its', 'range', 'of', 'movement', 'to', 'help', 'reduce', 'stiffness', 'and', 'improve', 'circulation'] ['avoid', 'exercise', 'a', 'painful', 'inflame', 'or', 'hot', 'joint', 'insteadgently', 'move', 'the', 'joint', 'through', 'its', 'range', 'of', 'movement', 'to', 'help', 'reduce', 'stiffness', 'and', 'improve', 'circulation']\n",
            "0 0.9523809523809523 ['you', 'attention', 'to', 'good', 'technique', 'and', 'try', 'to', 'move', 'smoothly', 'do', 'n’t', 'force', 'a', 'joint', 'beyond', 'a', 'comfortable', 'range', 'of', 'movement'] ['pay', 'attention', 'to', 'good', 'technique', 'and', 'try', 'to', 'move', 'smoothly', 'do', 'n’t', 'force', 'a', 'joint', 'beyond', 'a', 'comfortable', 'range', 'of', 'movement']\n",
            "0 0.9655172413793104 ['avoid', 'heavy', 'to', 'do', 'too', 'much', 'or', 'push', 'too', 'hard', 'too', 'soon', 'if', 'you', '’re', 'short', 'of', 'breath', 'or', 'in', 'pain', 'ease', 'back', 'on', 'the', 'intensity', 'of', 'your', 'exercise'] ['avoid', 'try', 'to', 'do', 'too', 'much', 'or', 'push', 'too', 'hard', 'too', 'soon', 'if', 'you', '’re', 'short', 'of', 'breath', 'or', 'in', 'pain', 'ease', 'back', 'on', 'the', 'intensity', 'of', 'your', 'exercise']\n",
            "0 0.9583333333333334 ['if', 'your', 'joint', 'feel', 'particularly', 'painful', 'after', 'for', 'longer', 'than', 'two', 'hours', 'after', 'an', 'exercise', 'session', 'reduce', 'the', 'intensity', 'of', 'your', 'next', 'exercise', 'session'] ['if', 'your', 'joint', 'feel', 'particularly', 'painful', 'afterwards', 'for', 'longer', 'than', 'two', 'hours', 'after', 'an', 'exercise', 'session', 'reduce', 'the', 'intensity', 'of', 'your', 'next', 'exercise', 'session']\n",
            "0 0.9523809523809523 ['if', 'with', 'friends', 'in', 'a', 'group', 'or', 'a', 'team', 'environment', 'can', 'be', 'helpful', 'if', 'you', 'find', 'it', 'difficult', 'to', 'get', 'motivate'] ['exercise', 'with', 'friends', 'in', 'a', 'group', 'or', 'a', 'team', 'environment', 'can', 'be', 'helpful', 'if', 'you', 'find', 'it', 'difficult', 'to', 'get', 'motivate']\n",
            "0 0.9444444444444444 ['try', 'exercise', 'have', 'many', 'health', 'benefit', 'for', 'people', 'with', 'musculoskeletal', 'condition', 'include', 'better', 'sleep', 'and', 'lower', 'stress', 'level'] ['regular', 'exercise', 'have', 'many', 'health', 'benefit', 'for', 'people', 'with', 'musculoskeletal', 'condition', 'include', 'better', 'sleep', 'and', 'lower', 'stress', 'level']\n",
            "0 0.9565217391304348 ['we', 'recommend', 'you', 'try', 'a', 'banana', 'mango', 'after', 'an', 'aerobic', 'workout', 'such', 'as', 'a', 'run', 'to', 'help', 'replenish', 'energy', 'store', 'in', 'your', 'muscle'] ['we', 'recommend', 'you', 'have', 'a', 'banana', 'mango', 'after', 'an', 'aerobic', 'workout', 'such', 'as', 'a', 'run', 'to', 'help', 'replenish', 'energy', 'store', 'in', 'your', 'muscle']\n",
            "0 0.9583333333333334 ['the', 'clinicians', 'be', 'better', 'at', 'absorb', 'minerals', 'from', 'foods', 'than', 'from', 'supplement', 'hence', 'food', 'be', 'recommend', 'as', 'a', 'good', 'source', 'for', 'minerals', 'and', 'vitamins'] ['the', 'body', 'be', 'better', 'at', 'absorb', 'minerals', 'from', 'foods', 'than', 'from', 'supplement', 'hence', 'food', 'be', 'recommend', 'as', 'a', 'good', 'source', 'for', 'minerals', 'and', 'vitamins']\n",
            "0 1.0 ['try', 'take', 'calcium', 'citrate', 'or', 'calcium', 'carbonate', 'supplement', 'or', 'try', 'other', 'source', 'of', 'calcium'] ['try', 'take', 'calcium', 'citrate', 'or', 'calcium', 'carbonate', 'supplement', 'or', 'try', 'other', 'source', 'of', 'calcium']\n",
            "0 1.0 ['vitamin', 'd', 'be', 'require', 'for', 'appropriate', 'absorption', 'of', 'calcium'] ['vitamin', 'd', 'be', 'require', 'for', 'appropriate', 'absorption', 'of', 'calcium']\n",
            "0 1.0 ['try', 'have', 'at', 'least', '–', 'minutes', 'of', 'exposure', 'to', 'the', 'sun', 'every', 'day', 'to', 'increase', 'vitamin', 'd', 'in', 'the', 'body'] ['try', 'have', 'at', 'least', '–', 'minutes', 'of', 'exposure', 'to', 'the', 'sun', 'every', 'day', 'to', 'increase', 'vitamin', 'd', 'in', 'the', 'body']\n",
            "0 0.9523809523809523 ['try', 'can', 'check', 'the', 'vitamin', 'd', 'level', 'in', 'your', 'body', 'by', 'discuss', 'this', 'with', 'your', 'gp', 'and', 'have', 'a', 'blood', 'test'] ['you', 'can', 'check', 'the', 'vitamin', 'd', 'level', 'in', 'your', 'body', 'by', 'discuss', 'this', 'with', 'your', 'gp', 'and', 'have', 'a', 'blood', 'test']\n",
            "0 0.9523809523809523 ['the', 'recommend', 'use', 'small', 'amount', 'of', 'different', 'cook', 'oil', 'with', 'a', 'lowfat', 'meal', 'plan', 'to', 'reduce', 'the', 'risk', 'of', 'heart', 'disease'] ['we', 'recommend', 'use', 'small', 'amount', 'of', 'different', 'cook', 'oil', 'with', 'a', 'lowfat', 'meal', 'plan', 'to', 'reduce', 'the', 'risk', 'of', 'heart', 'disease']\n",
            "0 0.9696969696969697 ['it', 'advise', 'you', 'discuss', 'with', 'your', 'nutritionist', 'to', 'know', 'exactly', 'what', 'percentage', 'of', 'your', 'total', 'calorie', 'intake', 'should', 'come', 'from', 'fat', 'as', 'per', 'your', 'need', 'take', 'into', 'account', 'your', 'medical', 'history', 'and', 'fitness'] ['we', 'advise', 'you', 'discuss', 'with', 'your', 'nutritionist', 'to', 'know', 'exactly', 'what', 'percentage', 'of', 'your', 'total', 'calorie', 'intake', 'should', 'come', 'from', 'fat', 'as', 'per', 'your', 'need', 'take', 'into', 'account', 'your', 'medical', 'history', 'and', 'fitness']\n",
            "0 0.9411764705882353 ['eat', 'recommend', 'you', 'try', 'include', 'monounsaturated', 'and', 'polyunsaturated', 'fat', 'in', 'your', 'diet', 'which', 'be', 'heart', 'healthy', 'fat'] ['we', 'recommend', 'you', 'try', 'include', 'monounsaturated', 'and', 'polyunsaturated', 'fat', 'in', 'your', 'diet', 'which', 'be', 'heart', 'healthy', 'fat']\n",
            "0 0.967741935483871 ['try', 'suggest', 'that', 'consume', 'a', 'combination', 'of', 'carbohydrates', 'such', 'as', 'fruit', 'juice', 'and', 'a', 'protein', 'such', 'as', 'milk', 'within', 'minutes', 'of', 'your', 'workout', 'this', 'allow', 'your', 'body', 'to', 'best', 'utilise', 'protein'] ['research', 'suggest', 'that', 'consume', 'a', 'combination', 'of', 'carbohydrates', 'such', 'as', 'fruit', 'juice', 'and', 'a', 'protein', 'such', 'as', 'milk', 'within', 'minutes', 'of', 'your', 'workout', 'this', 'allow', 'your', 'body', 'to', 'best', 'utilise', 'protein']\n",
            "BLEU_SCORE1: 0.4596904676020052 Precision: 0.5527765208596511 Recall: 0.5183230765127406 F1_Score: 0.5295769509258413 Meteor: 0.44655928379976473 PPL: 5.246874839198372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3590400815010071"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c80XdnV-tawV",
        "outputId": "801592f5-d8e0-444d-ec7f-192de5765132"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class ModelWithTemperature(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(ModelWithTemperature, self).__init__()\n",
        "        self.model = model\n",
        "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
        "\n",
        "    def forward(self, inputs, input_mask, targets, targets_mask):\n",
        "        logits = self.model(inputs, input_mask, targets, targets_mask)\n",
        "        return self.temperature_scale(logits)\n",
        "\n",
        "    def temperature_scale(self, logits):\n",
        "        # Expand temperature to match the size of logits\n",
        "        temperature = self.temperature.unsqueeze(1).expand(logits.size())\n",
        "        return logits / temperature\n",
        "\n",
        "    # This function probably should live outside of this class, but whatever\n",
        "    def set_temperature(self, valid_loader):\n",
        "        self.cuda()\n",
        "        ece_criterion = _ECELoss().cuda()\n",
        "        nll_criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "        # First: collect all the logits and labels for the validation set\n",
        "        logits_list = []\n",
        "        labels_list = []\n",
        "        with torch.no_grad():\n",
        "            for i, pair in enumerate(valid_loader):\n",
        "    \n",
        "                input = pair.Question.cuda()\n",
        "                label = pair.Answer.cuda()\n",
        "            #for input, label in valid_loader:\n",
        "                input = input.cuda()\n",
        "                label = label.cuda()\n",
        "                label = label[:, 1:]\n",
        "                input_mask, label_mask = create_masks(input, label)\n",
        "                logits = self.model(input, input_mask, label, label_mask)\n",
        "                logits_list.append(logits)\n",
        "                labels_list.append(label)\n",
        "            logits = torch.cat(logits_list).cuda()\n",
        "            labels = torch.cat(labels_list).cuda()\n",
        "            \n",
        "\n",
        "        # Next: optimize the temperature w.r.t. NLL\n",
        "        init_temp = self.temperature.clone()\n",
        "        optimizer = optim.LBFGS([self.temperature], lr=0.01, max_iter=50)\n",
        "\n",
        "        def eval():\n",
        "            labels_loss = labels.reshape(-1)\n",
        "            loss = nll_criterion(self.temperature_scale(logits.view(-1, ntokens)), labels_loss)\n",
        "            loss.backward()\n",
        "            return loss\n",
        "        optimizer.step(eval)\n",
        "\n",
        "        # CalculateECE after temperature scaling\n",
        "        labels_loss = labels.reshape(-1)\n",
        "        after_temperature_ece = ece_criterion(self.temperature_scale(logits.view(-1,ntokens )), labels_loss).item()\n",
        "        print('Initial temperature: %.3f, Optimal temperature: %.3f' % (init_temp, self.temperature.item()))\n",
        "        return self\n",
        "\n",
        "class _ECELoss(nn.Module):\n",
        "    def __init__(self, n_bins=15):\n",
        "        \"\"\"\n",
        "        n_bins (int): number of confidence interval bins\n",
        "        \"\"\"\n",
        "        super(_ECELoss, self).__init__()\n",
        "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "        self.bin_lowers = bin_boundaries[:-1]\n",
        "        self.bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        softmaxes = F.softmax(logits, dim=1)\n",
        "        confidences, predictions = torch.max(softmaxes, 1)\n",
        "        accuracies = predictions.eq(labels)\n",
        "        ece = torch.zeros(1, device=logits.device)\n",
        "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
        "            # Calculated |confidence - accuracy| in each bin\n",
        "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
        "            prop_in_bin = in_bin.float().mean()\n",
        "            if prop_in_bin.item() > 0:\n",
        "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "        return ece\n",
        "\n",
        "def evaluation(model, test_loader):\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    logits_list = []\n",
        "    labels_list = []\n",
        "    all_blue = []\n",
        "    all_acc = []\n",
        "    all_rec = []\n",
        "    all_f_score = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        #for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        for i, pair in enumerate(test_loader):\n",
        "    \n",
        "            inputs = pair.Question\n",
        "            targets = pair.Answer\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            targets = targets[:, 1:]\n",
        "            input_mask, targets_mask = create_masks(inputs, targets)\n",
        "            outputs = model(inputs, input_mask, targets, targets_mask)\n",
        "            logits_list.append(outputs)\n",
        "            labels_list.append(targets)\n",
        "            target_loss = targets.reshape(-1)\n",
        "            loss = criterion(outputs.view(-1, ntokens), target_loss)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(2)\n",
        "        \n",
        "            for idx in range(predicted.shape[0]):\n",
        "                \n",
        "                pred_sentence= prediction_ids2sentence(predicted[idx]).split()\n",
        "                gt=prediction_ids2sentence(targets[idx]).split()\n",
        "                BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "                reference_set = set(gt)\n",
        "                test_set = set(pred_sentence)\n",
        "                #rec = recall(reference_set, test_set)\n",
        "                #f_score = f_measure(reference_set, test_set)\n",
        "                all_blue.append(BLEU_1)\n",
        "                #all_rec.append(rec)\n",
        "                #all_f_score.append(f_score)\n",
        "    logits_all = torch.cat(logits_list).cuda()\n",
        "    labels_all = torch.cat(labels_list).cuda()\n",
        "    \n",
        "    return np.mean(all_blue),logits_all, labels_all\n",
        "\n",
        "def evaluate_matrics(transformer,test_loader):\n",
        "    sum_loss = 0\n",
        "    all_blue1 = []\n",
        "    all_blue2 = []\n",
        "    all_blue3 = []\n",
        "    all_blue4 = []\n",
        "\n",
        "    all_acc = []\n",
        "    all_prec = []\n",
        "    all_rec = []\n",
        "    all_f_score = []\n",
        "    all_meteor = []\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    #rev_word_map = {v: k for k, v in word_map_all.items()}\n",
        "    transformer.eval()\n",
        "    for i, pair in enumerate(test_loader):\n",
        "        #print(i)\n",
        "    \n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        \n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        reply_target_mask = reply_target.reshape(-1)\n",
        "        loss = criterion(out.view(-1, ntokens), reply_target_mask)\n",
        "        #loss = criterion(out, reply_target, reply_target_mask)\n",
        "        sum_loss += loss.item()\n",
        "        #loss = criterion(out, reply_target, reply_target_mask)\n",
        "        _, next = torch.max(out, dim = 2)# 2x51\n",
        "        #print(\"next\",next.size(),\"next0\",next[0].size(),\"next1\",next[1].size())\n",
        "        for idx in range(next.shape[0]):\n",
        "            pred_sentence= prediction_ids2sentence(next[idx]).split()\n",
        "            \n",
        "            gt=prediction_ids2sentence(reply_target[idx]).split()\n",
        "            BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "            #BLEU_2 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 0, 0))\n",
        "            #BLEU_3 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 0))\n",
        "            #BLEU_4 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 1))\n",
        "            #print (type(gt),type(pred_sentence))\n",
        "            reference_set = set(gt)\n",
        "            test_set = set(pred_sentence)\n",
        "            prec = precision(reference_set, test_set)\n",
        "            rec = recall(reference_set, test_set)\n",
        "            f_score = f_measure(reference_set, test_set)\n",
        "            meteor = single_meteor_score( str(gt), str(pred_sentence))\n",
        "            all_blue1.append(BLEU_1)\n",
        "            #all_blue2.append(BLEU_2)\n",
        "            #all_blue3.append(BLEU_3)\n",
        "            #all_blue4.append(BLEU_4)\n",
        "            all_prec.append(prec)\n",
        "            all_rec.append(rec)\n",
        "            all_f_score.append(f_score)\n",
        "            all_meteor.append(meteor)\n",
        "            #\"Recall:\",np.mean(all_rec),\n",
        "    pre = np.mean(all_prec)\n",
        "    recall_score = np.mean(all_rec)\n",
        "    f1 = np.mean(all_f_score)\n",
        "    met = np.mean(all_meteor)\n",
        "    ppl = math.exp(sum_loss/i)\n",
        "\n",
        "    print(\"BLEU_SCORE1:\",np.mean(all_blue1), \"Precision:\",np.mean(all_prec), \"Recall:\",np.mean(all_rec),\"F1_Score:\",np.mean(all_f_score), \"Meteor:\",np.mean(all_meteor),\"PPL:\",math.exp(sum_loss/i))\n",
        "\n",
        "seed_everything()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens)\n",
        "model = model.to(device)\n",
        "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_models_state_dict_SSL.pth'),strict=False)\n",
        "model.eval()\n",
        "ece_criterion = _ECELoss().to(device)\n",
        "bleu, logits_all, labels_all = evaluation(model, test_loader)\n",
        "logits_all = logits_all.view(-1,ntokens)\n",
        "labels_all = labels_all.view(-1)\n",
        "temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
        "print('Before TS- bleu:%.3f, bef ece:%.5f'%(bleu,temperature_ece))\n",
        "\n",
        "model_ts = ModelWithTemperature(model)\n",
        "model_ts.set_temperature(test_loader)\n",
        "bleu, logits_all, labels_all = evaluation(model_ts, test_loader)\n",
        "logits_all = logits_all.view(-1,ntokens)\n",
        "labels_all = labels_all.view(-1)\n",
        "temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
        "print('After TS- bleu:%.3f,aft ece:%.5f'%(bleu,temperature_ece))\n",
        "#torch.save(model_ts.state_dict(), 'best_model_ts.pth')\n",
        "\n",
        "\n",
        "model_ts.load_state_dict(torch.load('/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_models_state_dict_ULMFIT_SSL_LS_TS1.pth'))\n",
        "ece_criterion = _ECELoss().to(device)\n",
        "bleu, logits_all, labels_all = evaluation(model_ts, test_loader)\n",
        "logits_all = logits_all.view(-1,ntokens)\n",
        "labels_all = labels_all.view(-1)\n",
        "temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
        "\n",
        "evaluate_matrics(model_ts,test_loader)\n",
        "temperature_ece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Before TS- bleu:0.403, bef ece:0.37643\n",
            "Initial temperature: 1.500, Optimal temperature: 4.715\n",
            "After TS- bleu:0.403,aft ece:0.28847\n",
            "BLEU_SCORE1: 0.43474709718458887 Precision: 0.5074230352546428 Recall: 0.48772310286125936 F1_Score: 0.4940592358575808 Meteor: 0.42429769840374815 PPL: 7.622750266223356\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3752456307411194"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghcpnSRZ4nvt",
        "outputId": "113b33de-96ea-40e7-ec22-b35355b8b0ed"
      },
      "source": [
        "def evaluate_matrics(transformer,test_loader):\n",
        "    sum_loss = 0\n",
        "    all_blue1 = []\n",
        "    all_blue2 = []\n",
        "    all_blue3 = []\n",
        "    all_blue4 = []\n",
        "\n",
        "    all_acc = []\n",
        "    all_prec = []\n",
        "    all_rec = []\n",
        "    all_f_score = []\n",
        "    all_meteor = []\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    #rev_word_map = {v: k for k, v in word_map_all.items()}\n",
        "    transformer.eval()\n",
        "    for i, pair in enumerate(test_loader):\n",
        "        #print(i)\n",
        "    \n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        \n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        reply_target_mask = reply_target.reshape(-1)\n",
        "        loss = criterion(out.view(-1, ntokens), reply_target_mask)\n",
        "        #loss = criterion(out, reply_target, reply_target_mask)\n",
        "        sum_loss += loss.item()\n",
        "        #loss = criterion(out, reply_target, reply_target_mask)\n",
        "        _, next = torch.max(out, dim = 2)# 2x51\n",
        "        #print(\"next\",next.size(),\"next0\",next[0].size(),\"next1\",next[1].size())\n",
        "        for idx in range(next.shape[0]):\n",
        "            pred_sentence= prediction_ids2sentence(next[idx]).split()\n",
        "            \n",
        "            gt=prediction_ids2sentence(reply_target[idx]).split()\n",
        "            BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "            if BLEU_1 >= 0.94:\n",
        "                print(idx, BLEU_1,pred_sentence,gt)\n",
        "            #BLEU_2 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 0, 0))\n",
        "            #BLEU_3 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 0))\n",
        "            #BLEU_4 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 1))\n",
        "            #print (type(gt),type(pred_sentence))\n",
        "            reference_set = set(gt)\n",
        "            test_set = set(pred_sentence)\n",
        "            prec = precision(reference_set, test_set)\n",
        "            rec = recall(reference_set, test_set)\n",
        "            f_score = f_measure(reference_set, test_set)\n",
        "            meteor = single_meteor_score( str(gt), str(pred_sentence))\n",
        "            all_blue1.append(BLEU_1)\n",
        "            \n",
        "            #all_blue2.append(BLEU_2)\n",
        "            #all_blue3.append(BLEU_3)\n",
        "            #all_blue4.append(BLEU_4)\n",
        "            all_prec.append(prec)\n",
        "            all_rec.append(rec)\n",
        "            all_f_score.append(f_score)\n",
        "            all_meteor.append(meteor)\n",
        "            #\"Recall:\",np.mean(all_rec),\n",
        "    pre = np.mean(all_prec)\n",
        "    recall_score = np.mean(all_rec)\n",
        "    f1 = np.mean(all_f_score)\n",
        "    met = np.mean(all_meteor)\n",
        "    ppl = math.exp(sum_loss/i)\n",
        "\n",
        "    print(\"BLEU_SCORE1:\",np.mean(all_blue1), \"Precision:\",np.mean(all_prec), \"Recall:\",np.mean(all_rec),\"F1_Score:\",np.mean(all_f_score), \"Meteor:\",np.mean(all_meteor),\"PPL:\",math.exp(sum_loss/i))\n",
        "\n",
        "evaluate_matrics(model_ts,test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 0.9411764705882353 ['try', 'up', 'and', 'move', 'about', 'gently', 'for', 'a', 'short', 'period', 'every', 'hour', 'would', 'help', 'relieve', 'muscle', 'stiffness'] ['stand', 'up', 'and', 'move', 'about', 'gently', 'for', 'a', 'short', 'period', 'every', 'hour', 'would', 'help', 'relieve', 'muscle', 'stiffness']\n",
            "0 0.9523809523809523 ['try', 'most', 'back', 'problems', 'start', 'for', 'no', 'obvious', 'reason', 'back', 'pain', 'can', 'be', 'cause', 'by', 'stay', 'in', 'one', 'position', 'too', 'long'] ['although', 'most', 'back', 'problems', 'start', 'for', 'no', 'obvious', 'reason', 'back', 'pain', 'can', 'be', 'cause', 'by', 'stay', 'in', 'one', 'position', 'too', 'long']\n",
            "0 0.9523809523809523 ['try', 'attention', 'to', 'good', 'technique', 'and', 'try', 'to', 'move', 'smoothly', 'do', 'n’t', 'force', 'a', 'joint', 'beyond', 'a', 'comfortable', 'range', 'of', 'movement'] ['pay', 'attention', 'to', 'good', 'technique', 'and', 'try', 'to', 'move', 'smoothly', 'do', 'n’t', 'force', 'a', 'joint', 'beyond', 'a', 'comfortable', 'range', 'of', 'movement']\n",
            "0 0.9655172413793104 ['try', 'try', 'to', 'do', 'too', 'much', 'or', 'push', 'too', 'hard', 'too', 'soon', 'if', 'you', '’re', 'short', 'of', 'breath', 'or', 'in', 'pain', 'ease', 'back', 'on', 'the', 'intensity', 'of', 'your', 'exercise'] ['avoid', 'try', 'to', 'do', 'too', 'much', 'or', 'push', 'too', 'hard', 'too', 'soon', 'if', 'you', '’re', 'short', 'of', 'breath', 'or', 'in', 'pain', 'ease', 'back', 'on', 'the', 'intensity', 'of', 'your', 'exercise']\n",
            "0 0.9583333333333334 ['try', 'will', 'help', 'you', 'build', 'muscle', 'strength', 'provide', 'stability', 'to', 'your', 'joint', 'improve', 'your', 'bone', 'health', 'and', 'improve', 'your', 'ability', 'to', 'perform', 'daily', 'task'] ['exercise', 'will', 'help', 'you', 'build', 'muscle', 'strength', 'provide', 'stability', 'to', 'your', 'joint', 'improve', 'your', 'bone', 'health', 'and', 'improve', 'your', 'ability', 'to', 'perform', 'daily', 'task']\n",
            "0 0.9545454545454546 ['try', 'a', 'mediumsize', 'mango', 'and', 'half', 'a', 'banana', 'have', 'the', 'same', 'calories', 'as', 'a', 'tennisball', 'size', 'apple', 'opt', 'for', 'an', 'apple', 'instead'] ['half', 'a', 'mediumsize', 'mango', 'and', 'half', 'a', 'banana', 'have', 'the', 'same', 'calories', 'as', 'a', 'tennisball', 'size', 'apple', 'opt', 'for', 'an', 'apple', 'instead']\n",
            "0 0.9565217391304348 ['try', 'intake', 'of', 'saturate', 'fat', 'such', 'as', 'butter', 'clarify', 'butterghee', 'palm', 'oil', 'increase', 'the', 'risk', 'of', 'heart', 'disease', 'include', 'a', 'high', 'fat', 'diet'] ['increase', 'intake', 'of', 'saturate', 'fat', 'such', 'as', 'butter', 'clarify', 'butterghee', 'palm', 'oil', 'increase', 'the', 'risk', 'of', 'heart', 'disease', 'include', 'a', 'high', 'fat', 'diet']\n",
            "BLEU_SCORE1: 0.43474709718458887 Precision: 0.5074230352546428 Recall: 0.48772310286125936 F1_Score: 0.4940592358575808 Meteor: 0.42429769840374815 PPL: 7.622750266223356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbxDOu_t6Ptt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}