{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_GloVe_SSL.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AoShuang92/ULMFiT_and_SD_with_Calibration_for_Medical_Dialogue_System/blob/main/Transformer_GloVe_SSL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7-Yal7-o0vN",
        "outputId": "e2abd367-6c9b-4796-a406-08cbf648c5f7"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "#system\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from collections import Counter\n",
        "import json\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext import data\n",
        "from nltk.metrics import accuracy, precision, recall, f_measure\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "\n",
        "def seed_everything(seed=27):\n",
        "  #random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaC-MomvBDAv",
        "outputId": "e21c8822-fbbd-4cb8-c5c7-88fe84c07dee"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJkwP6eaB589",
        "outputId": "c9deb142-385c-47dd-d260-f178091501b6"
      },
      "source": [
        "!pip install \"nltk==3.4.5\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\r\u001b[K     |▎                               | 10kB 11.8MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 16.5MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 15.1MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 14.3MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 12.1MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 12.6MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 10.1MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81kB 10.9MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 11.8MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102kB 11.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112kB 11.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122kB 11.1MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 11.1MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143kB 11.1MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153kB 11.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163kB 11.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174kB 11.1MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 11.1MB/s eta 0:00:01\r\u001b[K     |████▎                           | 194kB 11.1MB/s eta 0:00:01\r\u001b[K     |████▌                           | 204kB 11.1MB/s eta 0:00:01\r\u001b[K     |████▊                           | 215kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 225kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 235kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 245kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 256kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 266kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 276kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 286kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 296kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 307kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 317kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 327kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 337kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 348kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 358kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 368kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 378kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 389kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 399kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 409kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 419kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 430kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 440kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 450kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 460kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 471kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 481kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 491kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 501kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 512kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 522kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 532kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 542kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 552kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 563kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 573kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 583kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 593kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 604kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 614kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 624kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 634kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 645kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 655kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 665kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 675kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 686kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 696kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 706kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 716kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 727kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 737kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 747kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 757kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 768kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 778kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 788kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 798kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 808kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 819kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 829kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 839kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 849kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 860kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 870kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 880kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 890kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 901kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 911kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 921kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 931kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 942kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 952kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 962kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 972kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 983kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 993kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.0MB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.0MB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.0MB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.0MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1MB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1MB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1MB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.1MB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.1MB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2MB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2MB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2MB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.2MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.3MB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.3MB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.3MB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.3MB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.3MB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.3MB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.3MB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.4MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.4MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.4MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.4MB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.4MB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 11.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5) (1.15.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449908 sha256=38616ff9c9ec7cbd052eae5d1c59556c0c62a093b706d3b857ac79485932697e\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFHqt1eepjjr"
      },
      "source": [
        "max_length = 35\n",
        "#train_dir = \"/content/drive/MyDrive/chatbot/combined_qa_train_ID.csv\"\n",
        "#test_dir = \"/content/drive/MyDrive/chatbot/combined_qa_test_50_ID.csv\"\n",
        "train_dir = \"/content/drive/MyDrive/chatbot/combined_qa_train_ID.csv\"\n",
        "test_dir = \"/content/drive/MyDrive/chatbot/combined_qa_test_200_ID.csv\"\n",
        "batch_size = 4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def remove_unnecessary(text):\n",
        "    #remove_URL\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    text = url.sub('', text)\n",
        "\n",
        "    #remove_html\n",
        "    html = re.compile(r'<.*?>')\n",
        "    text = html.sub('', text)\n",
        "\n",
        "    #remove @\n",
        "    text = re.sub('@[^\\s]+','',text)\n",
        "\n",
        "    #remove_emoji\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    #Removes integers \n",
        "    text = ''.join([i for i in text if not i.isdigit()])         \n",
        "    \n",
        "    #remove_punct\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(table)\n",
        "\n",
        "    #Replaces contractions from a string to their equivalents \n",
        "    contraction_patterns = [(r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), \n",
        "                            (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
        "                            (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'),\n",
        "                            (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), \n",
        "                            (r'dont', 'do not'), (r'wont', 'will not')]\n",
        "    \n",
        "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
        "    for (pattern, repl) in patterns:\n",
        "        text, _= re.subn(pattern, repl, text)\n",
        "\n",
        "    #lemmatize_sentence\n",
        "    sentence_words = text.split(' ')\n",
        "    new_sentence_words = list()\n",
        "    \n",
        "    for sentence_word in sentence_words:\n",
        "        sentence_word = sentence_word.replace('#', '')\n",
        "        new_sentence_word = WordNetLemmatizer().lemmatize(sentence_word.lower(), wordnet.VERB)\n",
        "        new_sentence_words.append(new_sentence_word)\n",
        "        \n",
        "    new_sentence = ' '.join(new_sentence_words)\n",
        "    new_sentence = new_sentence.strip()\n",
        "\n",
        "    return new_sentence.lower()\n",
        "\n",
        "def prepare_csv(train,test):\n",
        "    # idx = np.arange(df_train.shape[0])    \n",
        "    # np.random.shuffle(idx)\n",
        "    # val_size = int(len(idx) * val_ratio)\n",
        "    if not os.path.exists('cache'): # cache is tem memory file \n",
        "        os.makedirs('cache')\n",
        "    \n",
        "    train_temp = train[['Question', 'Answer']].to_csv(\n",
        "        'cache/dataset_train.csv', index=True)\n",
        "    \n",
        "    test_temp = test[['Question', 'Answer']].to_csv(\n",
        "        'cache/dataset_val.csv', index=True) \n",
        "    return  train_temp,  test_temp\n",
        "\n",
        "def get_iterator(dataset, batch_size, train=True,\n",
        "                 shuffle=True, repeat=False, device=None): \n",
        "    dataset_iter = data.Iterator(\n",
        "        dataset, batch_size=batch_size, device=device,\n",
        "        train=train, shuffle=shuffle, repeat=repeat,\n",
        "        sort=False)  \n",
        "    return dataset_iter\n",
        "\n",
        "def get_dataset(fix_length=max_length, lower=False, vectors=None,train_dir = train_dir, test_dir = test_dir, batch_size=batch_size, device=None): \n",
        "    train = pd.read_csv(train_dir,error_bad_lines=False)\n",
        "    test =  pd.read_csv(test_dir,error_bad_lines=False)\n",
        "    train['Question'] = train['Question'].apply(lambda x: remove_unnecessary(x))\n",
        "    train['Answer'] = train['Answer'].apply(lambda x: remove_unnecessary(x))\n",
        "    test['Question'] = test['Question'].apply(lambda x: remove_unnecessary(x))\n",
        "    test['Answer'] = test['Answer'].apply(lambda x: remove_unnecessary(x))\n",
        "    train_temp,  test_temp = prepare_csv(train,test)\n",
        "    if vectors is not None:\n",
        "        lower=True\n",
        "\n",
        "    TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"spacy\"),init_token='<sos>',eos_token='<eos>',lower=True,batch_first=True, \n",
        "                      fix_length=fix_length)\n",
        "    ID = data.Field(use_vocab=False, sequential=False, dtype=torch.float16)  \n",
        "    train_temps = data.TabularDataset(\n",
        "        path='/content/cache/dataset_train.csv', format='csv', skip_header=True,\n",
        "        fields=[(\"ID\",ID),('Question', TEXT), ('Answer', TEXT)]) \n",
        "    test_temps = data.TabularDataset(\n",
        "        path='/content/cache/dataset_val.csv', format='csv', skip_header=True,\n",
        "        fields=[(\"ID\",ID),('Question', TEXT), ('Answer', TEXT)]) \n",
        "\n",
        "    TEXT.build_vocab(train_temps,test_temps, vectors=GloVe(name='6B', dim=300))\n",
        "    ID.build_vocab(train_temps, test_temps)\n",
        "    word_embeddings = TEXT.vocab.vectors\n",
        "    vocab_size = len(TEXT.vocab)\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    print(\"vocab_size_and_ntokens:\",vocab_size,ntokens)\n",
        "    train_loader = get_iterator(train_temps, batch_size=batch_size, \n",
        "                                train=True, shuffle=True,\n",
        "                                repeat=False,device=None)\n",
        "    test_loader = get_iterator(test_temps, batch_size=batch_size, \n",
        "                            train=False, shuffle=False,\n",
        "                            repeat=False, device=None)\n",
        "    print('Train samples:%d'%(len(train_temps)), 'Valid samples:%d'%(len(test_temps)),'Train minibatch nb:%d'%(len(train_loader)),\n",
        "            'Valid minibatch nb:%d'%(len(test_loader)))\n",
        "    return vocab_size, word_embeddings, ntokens, train_loader, test_loader, TEXT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrQV-PKdqHOO",
        "outputId": "e649f128-9883-4055-b8d3-e92f47f71cf9"
      },
      "source": [
        "vocab_size, word_embeddings, ntokens, train_loader, test_loader, TEXT = get_dataset(fix_length=max_length,train_dir = train_dir, test_dir = test_dir, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:31, 2.20MB/s]                           \n",
            "100%|█████████▉| 399985/400000 [00:51<00:00, 7949.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "vocab_size_and_ntokens: 1938 1938\n",
            "Train samples:1001 Valid samples:200 Train minibatch nb:251 Valid minibatch nb:50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7PgYwYcqNP1"
      },
      "source": [
        "def create_masks(question, reply_input):\n",
        "    \n",
        "    def subsequent_mask(size):\n",
        "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "        return mask.unsqueeze(0)\n",
        "    \n",
        "    question_mask = question!=0\n",
        "    question_mask = question_mask.to(device)\n",
        "    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n",
        "     \n",
        "    reply_input_mask = reply_input!=0\n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n",
        "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n",
        "    \n",
        "    return question_mask, reply_input_mask\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(max_length, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len]\n",
        "    return data, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaD0bzWAsTme"
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements embeddings of the words and adds their positional encodings. \n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, max_len = max_length):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pe = self.create_positinal_encoding(max_len, self.d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def create_positinal_encoding(self, max_len, d_model):\n",
        "        pe = torch.zeros(max_len, d_model).to(device)\n",
        "        for pos in range(max_len):   # for each position of the word\n",
        "            for i in range(0, d_model, 2):   # for each dimension of the each position\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)   # include the batch size\n",
        "        return pe\n",
        "        \n",
        "    def forward(self, encoded_words):\n",
        "        embedding = self.embed(encoded_words) * math.sqrt(self.d_model)\n",
        "        #print(\"embedding\",embedding.size(),encoded_words.size())\n",
        "        #print(\"pe\",self.pe.size())\n",
        "        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n",
        "        embedding = self.dropout(embedding)\n",
        "        return embedding\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, heads, d_model):\n",
        "        \n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % heads == 0\n",
        "        self.d_k = d_model // heads\n",
        "        self.heads = heads\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        self.concat = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        query, key, value of shape: (batch_size, max_len, 512)\n",
        "        mask of shape: (batch_size, 1, 1, max_words)\n",
        "        \"\"\"\n",
        "        # (batch_size, max_len, 512)\n",
        "        query = self.query(query)\n",
        "        key = self.key(key)        \n",
        "        value = self.value(value)   \n",
        "        \n",
        "        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
        "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        \n",
        "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
        "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
        "        #scores = torch.matmul(query, key.permute(2,1,0,0)) / math.sqrt(query.size(-1))\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n",
        "        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n",
        "        weights = self.dropout(weights)\n",
        "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        context = torch.matmul(weights, value)\n",
        "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n",
        "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
        "        # (batch_size, max_len, h * d_k)\n",
        "        interacted = self.concat(context)\n",
        "        return interacted\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, middle_dim = 2048):\n",
        "        super(FeedForward, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
        "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.fc1(x))\n",
        "        out = self.fc2(self.dropout(out))\n",
        "        return out\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, heads):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, embeddings, mask):\n",
        "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
        "        interacted = self.layernorm(interacted + embeddings)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        encoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return encoded\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, heads):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
        "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
        "        query = self.layernorm(query + embeddings)\n",
        "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
        "        interacted = self.layernorm(interacted + query)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        decoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return decoded\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):    \n",
        "    def __init__(self, d_model, heads, num_layers, ntokens):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = ntokens\n",
        "        self.embed = Embeddings(self.vocab_size, d_model)#max_len\n",
        "        self.embed_dec = Embeddings(self.vocab_size, d_model,max_length)\n",
        "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        self.logit = nn.Linear(d_model, self.vocab_size)   \n",
        "        \n",
        "    def encode(self, src_words, src_mask):\n",
        "        src_embeddings = self.embed(src_words)\n",
        "        for layer in self.encoder:\n",
        "            src_embeddings = layer(src_embeddings, src_mask)\n",
        "        return src_embeddings\n",
        "    \n",
        "    def decode(self, target_words, target_mask, src_embeddings, src_mask):\n",
        "        tgt_embeddings = self.embed_dec(target_words)\n",
        "        for layer in self.decoder:\n",
        "            tgt_embeddings = layer(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
        "        return tgt_embeddings\n",
        "        \n",
        "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
        "        encoded = self.encode(src_words, src_mask)\n",
        "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
        "        out = F.log_softmax(self.logit(decoded), dim = 2)\n",
        "        return out\n",
        "\n",
        "class AdamWarmup:\n",
        "    \n",
        "    def __init__(self, model_size, warmup_steps, optimizer):\n",
        "        \n",
        "        self.model_size = model_size\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.optimizer = optimizer\n",
        "        self.current_step = 0\n",
        "        self.lr = 0\n",
        "        \n",
        "    def get_lr(self):\n",
        "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
        "        \n",
        "    def step(self):\n",
        "        # Increment the number of steps each time we call the step function\n",
        "        self.current_step += 1\n",
        "        lr = self.get_lr()\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        # update the learning rate\n",
        "        self.lr = lr\n",
        "        self.optimizer.step()\n",
        "\n",
        "class LossWithLS(nn.Module):\n",
        "\n",
        "    def __init__(self, size, smooth):\n",
        "        super(LossWithLS, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n",
        "        self.confidence = 1.0 - smooth\n",
        "        self.smooth = smooth\n",
        "        self.size = size\n",
        "        \n",
        "    def forward(self, prediction, target, mask):\n",
        "        \"\"\"\n",
        "        prediction of shape: (batch_size, max_words, vocab_size)\n",
        "        target and mask of shape: (batch_size, max_words)\n",
        "        \"\"\"\n",
        "        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n",
        "        target = target.contiguous().view(-1)   # (batch_size * max_words)\n",
        "        mask = mask.float()\n",
        "        mask = mask.view(-1)       # (batch_size * max_words)\n",
        "        labels = prediction.data.clone()\n",
        "        labels.fill_(self.smooth / (self.size - 1))\n",
        "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n",
        "        loss = (loss.sum(1) * mask).sum() / mask.sum()\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qbLMn34sYmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59eee0df-1664-48c8-f6e6-ed1eae0ada40"
      },
      "source": [
        "def train(train_loader, transformer, criterion, epoch):    \n",
        "    transformer.train()\n",
        "    sum_loss = 0\n",
        "    count = 0\n",
        "    \n",
        "    for i, pair in enumerate(train_loader): \n",
        "    #for i, (question, reply) in enumerate(train_loader):     \n",
        "        #samples = question.shape[0]\n",
        "        # Move to device\n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        \n",
        "        # Prepare Target Data\n",
        "        #print(\"pair\",pair, type(pair),len(pair))\n",
        "        #print(\"reply\",type(reply))\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "\n",
        "        # Create mask and add dimensions\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        reply_target = reply_target.reshape(-1)\n",
        "        loss = criterion(out.view(-1, ntokens), reply_target)\n",
        "        #loss = criterion(out, reply_target)\n",
        "        \n",
        "        # Backprop\n",
        "        transformer_optimizer.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        transformer_optimizer.step()\n",
        "        \n",
        "        #sum_loss += loss.item() * samples\n",
        "        #count += samples\n",
        "        #break\n",
        "        # if i % 100 == 0:\n",
        "        #     print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))\n",
        "\n",
        "def valid (test_loader,transformer): \n",
        "    all_blue = []\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    #rev_word_map = {v: k for k, v in word_embeddings.items()}\n",
        "    transformer.eval()\n",
        "    for i, pair in enumerate(test_loader):\n",
        "    #for i, (question, reply) in enumerate(test_loader):\n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        #loss = criterion(out, reply_target, reply_target_mask)\n",
        "        _, next = torch.max(out, dim = 2)# 2x51\n",
        "        for idx in range(next.shape[0]):\n",
        "            pred_sentence= prediction_ids2sentence(next[idx]).split()\n",
        "            gt=prediction_ids2sentence(reply_target[idx]).split()\n",
        "            BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "            # BLEU_2 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 0, 0))\n",
        "            # BLEU_3 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 0))\n",
        "            # BLEU_4 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 1))\n",
        "            all_blue.append(BLEU_1)\n",
        "    #print(\"BLEU_score:\",np.mean(all_blue))\n",
        "    return np.mean(all_blue)\n",
        "\n",
        "def evaluate(transformer, question, question_mask, max_len):\n",
        "    \"\"\"\n",
        "    Performs Greedy Decoding with a batch size of 1\n",
        "    \"\"\"\n",
        "    #rev_word_map = {v: k for k, v in word_embeddings.items()}\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    transformer.eval()\n",
        "    start_token = word_map['<sos>']\n",
        "    encoded = transformer.encode(question, question_mask)\n",
        "    words = torch.LongTensor([[start_token]]).to(device)\n",
        "    next_word = -22\n",
        "    while next_word != word_map['<eos>']:\n",
        "    #for step in range(max_len - 1):\n",
        "        size = words.shape[1]\n",
        "        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
        "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
        "        predictions = transformer.logit(decoded[:, -1])\n",
        "        _, next_word = torch.max(predictions, dim = 1)\n",
        "        next_word = next_word.item()\n",
        "        if next_word == word_map['<eos>'] or words.shape[1]==(max_len+1):\n",
        "            break\n",
        "        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n",
        "        \n",
        "    # Construct Sentence\n",
        "    if words.dim() == 2:\n",
        "        words = words.squeeze(0)\n",
        "        words = words.tolist()\n",
        "        \n",
        "    sen_idx = [w for w in words if w not in {word_map['<sos>']}]\n",
        "    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n",
        "    \n",
        "\n",
        "    return sentence\n",
        "\n",
        "def prediction_ids2sentence(pred_ids):\n",
        "    #rev_word_map = {v: k for k, v in word_embeddings.items()}\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    #_, next = torch.max(out, dim = 2)\n",
        "    sen_idx = []\n",
        "    for w in pred_ids:\n",
        "        if w == word_map['<eos>']:\n",
        "            break\n",
        "        sen_idx.append(w)\n",
        "    #print(sen_idx)\n",
        "    sentence = ' '.join([rev_word_map[int(sen_idx[k])] for k in range(len(sen_idx))])\n",
        "    return sentence\n",
        "\n",
        "from nltk.metrics import accuracy, precision, recall, f_measure\n",
        "\n",
        "def evaluate_matrics(transformer,test_loader):\n",
        "    sum_loss = 0\n",
        "    all_blue1 = []\n",
        "    all_blue2 = []\n",
        "    all_blue3 = []\n",
        "    all_blue4 = []\n",
        "\n",
        "    all_acc = []\n",
        "    all_prec = []\n",
        "    all_rec = []\n",
        "    all_f_score = []\n",
        "    all_meteor = []\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    #rev_word_map = {v: k for k, v in word_map_all.items()}\n",
        "    transformer.eval()\n",
        "    for i, pair in enumerate(test_loader):\n",
        "        #print(i)\n",
        "    \n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        \n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        reply_target_mask = reply_target.reshape(-1)\n",
        "        loss = criterion(out.view(-1, ntokens), reply_target_mask)\n",
        "        #loss = criterion(out, reply_target, reply_target_mask)\n",
        "        sum_loss += loss.item()\n",
        "        #loss = criterion(out, reply_target, reply_target_mask)\n",
        "        _, next = torch.max(out, dim = 2)# 2x51\n",
        "        #print(\"next\",next.size(),\"next0\",next[0].size(),\"next1\",next[1].size())\n",
        "        for idx in range(next.shape[0]):\n",
        "            pred_sentence= prediction_ids2sentence(next[idx]).split()\n",
        "            \n",
        "            gt=prediction_ids2sentence(reply_target[idx]).split()\n",
        "            BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "            #BLEU_2 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 0, 0))\n",
        "            #BLEU_3 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 0))\n",
        "            #BLEU_4 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 1))\n",
        "            #print (type(gt),type(pred_sentence))\n",
        "            reference_set = set(gt)\n",
        "            test_set = set(pred_sentence)\n",
        "            prec = precision(reference_set, test_set)\n",
        "            rec = recall(reference_set, test_set)\n",
        "            f_score = f_measure(reference_set, test_set)\n",
        "            meteor = single_meteor_score( str(gt), str(pred_sentence))\n",
        "            all_blue1.append(BLEU_1)\n",
        "            #all_blue2.append(BLEU_2)\n",
        "            #all_blue3.append(BLEU_3)\n",
        "            #all_blue4.append(BLEU_4)\n",
        "            all_prec.append(prec)\n",
        "            all_rec.append(rec)\n",
        "            all_f_score.append(f_score)\n",
        "            all_meteor.append(meteor)\n",
        "            #\"Recall:\",np.mean(all_rec),\n",
        "    pre = np.mean(all_prec)\n",
        "    recall_score = np.mean(all_rec)\n",
        "    f1 = np.mean(all_f_score)\n",
        "    met = np.mean(all_meteor)\n",
        "    ppl = math.exp(sum_loss/i)\n",
        "\n",
        "    print(\"BLEU_SCORE1:\",np.mean(all_blue1), \"Precision:\",np.mean(all_prec), \"Recall:\",np.mean(all_rec),\"F1_Score:\",np.mean(all_f_score), \"Meteor:\",np.mean(all_meteor),\"PPL:\",math.exp(sum_loss/i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 399985/400000 [01:10<00:00, 7949.90it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_dUpT_rBPqO"
      },
      "source": [
        "class _ECELoss(nn.Module):\n",
        "    def __init__(self, n_bins=15):\n",
        "        \"\"\"\n",
        "        n_bins (int): number of confidence interval bins\n",
        "        \"\"\"\n",
        "        super(_ECELoss, self).__init__()\n",
        "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "        self.bin_lowers = bin_boundaries[:-1]\n",
        "        self.bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        softmaxes = F.softmax(logits, dim=1)\n",
        "        confidences, predictions = torch.max(softmaxes, 1)\n",
        "        accuracies = predictions.eq(labels)\n",
        "        ece = torch.zeros(1, device=logits.device)\n",
        "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
        "            # Calculated |confidence - accuracy| in each bin\n",
        "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
        "            prop_in_bin = in_bin.float().mean()\n",
        "            if prop_in_bin.item() > 0:\n",
        "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "        return ece\n",
        "\n",
        "def evaluation(model, test_loader):\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    logits_list = []\n",
        "    labels_list = []\n",
        "    all_blue = []\n",
        "    all_acc = []\n",
        "    all_rec = []\n",
        "    all_f_score = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        #for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        for i, pair in enumerate(test_loader):\n",
        "    \n",
        "            inputs = pair.Question\n",
        "            targets = pair.Answer\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            targets = targets[:, 1:]\n",
        "            input_mask, targets_mask = create_masks(inputs, targets)\n",
        "            outputs = model(inputs, input_mask, targets, targets_mask)\n",
        "            #print(outputs.shape)\n",
        "            logits_list.append(outputs)\n",
        "            labels_list.append(targets)\n",
        "            # target_loss = targets.reshape(-1)\n",
        "            # loss = criterion(outputs.view(-1, ntokens), target_loss)\n",
        "            # test_loss += loss.item()\n",
        "            _, predicted = outputs.max(2)\n",
        "        \n",
        "            for idx in range(predicted.shape[0]):\n",
        "                \n",
        "                pred_sentence= prediction_ids2sentence(predicted[idx]).split()\n",
        "                gt=prediction_ids2sentence(targets[idx]).split()\n",
        "                BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "                reference_set = set(gt)\n",
        "                test_set = set(pred_sentence)\n",
        "                #rec = recall(reference_set, test_set)\n",
        "                #f_score = f_measure(reference_set, test_set)\n",
        "                all_blue.append(BLEU_1)\n",
        "                #all_rec.append(rec)\n",
        "                #all_f_score.append(f_score)\n",
        "\n",
        "    logits_all = torch.cat(logits_list).cuda()\n",
        "    labels_all = torch.cat(labels_list).cuda()\n",
        "    \n",
        "    return np.mean(all_blue),logits_all, labels_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyNgTQMRse52",
        "outputId": "277a8630-2173-4b17-fc70-718e607f0dd8"
      },
      "source": [
        "seed_everything()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens)\n",
        "model = model.to(device)\n",
        "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "\n",
        "best_blue = 0\n",
        "best_epoch = 0\n",
        "for epoch in range(30):\n",
        "    train(train_loader, model, criterion, epoch)\n",
        "    blue_score = valid (test_loader,model)\n",
        "    if blue_score > best_blue:\n",
        "        best_blue = blue_score\n",
        "        best_epoch = epoch\n",
        "        state = {'epoch': epoch, 'transformer': model, 'transformer_optimizer': transformer_optimizer}\n",
        "        torch.save(state, 'best_chatbot_model_object.pth.tar')\n",
        "        torch.save(model.state_dict(), 'best_chatbot_models_state_dict.pth')\n",
        "    print('cur epoch:%d, cur blue:%.5f, best epoch:%d, best blue:%.5f'%(epoch,blue_score, best_epoch, best_blue))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cur epoch:0, cur blue:0.00000, best epoch:0, best blue:0.00000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cur epoch:1, cur blue:0.10309, best epoch:1, best blue:0.10309\n",
            "cur epoch:2, cur blue:0.11855, best epoch:2, best blue:0.11855\n",
            "cur epoch:3, cur blue:0.14021, best epoch:3, best blue:0.14021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cur epoch:4, cur blue:0.13862, best epoch:3, best blue:0.14021\n",
            "cur epoch:5, cur blue:0.16695, best epoch:5, best blue:0.16695\n",
            "cur epoch:6, cur blue:0.18724, best epoch:6, best blue:0.18724\n",
            "cur epoch:7, cur blue:0.18561, best epoch:6, best blue:0.18724\n",
            "cur epoch:8, cur blue:0.21922, best epoch:8, best blue:0.21922\n",
            "cur epoch:9, cur blue:0.22165, best epoch:9, best blue:0.22165\n",
            "cur epoch:10, cur blue:0.20948, best epoch:9, best blue:0.22165\n",
            "cur epoch:11, cur blue:0.20637, best epoch:9, best blue:0.22165\n",
            "cur epoch:12, cur blue:0.18017, best epoch:9, best blue:0.22165\n",
            "cur epoch:13, cur blue:0.21102, best epoch:9, best blue:0.22165\n",
            "cur epoch:14, cur blue:0.20863, best epoch:9, best blue:0.22165\n",
            "cur epoch:15, cur blue:0.20422, best epoch:9, best blue:0.22165\n",
            "cur epoch:16, cur blue:0.19815, best epoch:9, best blue:0.22165\n",
            "cur epoch:17, cur blue:0.17532, best epoch:9, best blue:0.22165\n",
            "cur epoch:18, cur blue:0.16452, best epoch:9, best blue:0.22165\n",
            "cur epoch:19, cur blue:0.17468, best epoch:9, best blue:0.22165\n",
            "cur epoch:20, cur blue:0.18058, best epoch:9, best blue:0.22165\n",
            "cur epoch:21, cur blue:0.20716, best epoch:9, best blue:0.22165\n",
            "cur epoch:22, cur blue:0.21147, best epoch:9, best blue:0.22165\n",
            "cur epoch:23, cur blue:0.18897, best epoch:9, best blue:0.22165\n",
            "cur epoch:24, cur blue:0.20801, best epoch:9, best blue:0.22165\n",
            "cur epoch:25, cur blue:0.20223, best epoch:9, best blue:0.22165\n",
            "cur epoch:26, cur blue:0.21537, best epoch:9, best blue:0.22165\n",
            "cur epoch:27, cur blue:0.20152, best epoch:9, best blue:0.22165\n",
            "cur epoch:28, cur blue:0.22464, best epoch:28, best blue:0.22464\n",
            "cur epoch:29, cur blue:0.21520, best epoch:28, best blue:0.22464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfWg-zh7ssgL",
        "outputId": "bb9aba8b-188a-409f-ecaa-d214c4f217e8"
      },
      "source": [
        "seed_everything()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens)\n",
        "model = model.to(device)\n",
        "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_models_state_dict.pth'))\n",
        "model = model.to(device)\n",
        "best_blue_SSL = 0\n",
        "best_epoch_SSL = 0\n",
        "for epoch_SSL in range(30):\n",
        "    train(train_loader, model, criterion, epoch_SSL)\n",
        "    blue_score_SSL = valid (test_loader,model)\n",
        "    if blue_score_SSL > best_blue_SSL:\n",
        "        best_blue_SSL = blue_score_SSL\n",
        "        best_epoch_SSL = epoch_SSL\n",
        "        state = {'epoch': epoch_SSL, 'transformer': model, 'transformer_optimizer': transformer_optimizer}\n",
        "        torch.save(state, '/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_model_object_SSL.pth.tar')\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_models_state_dict_SSL.pth')\n",
        "    print('cur epoch:%d, cur blue:%.5f, best epoch:%d, best blue:%.5f'%(epoch_SSL,blue_score_SSL, best_epoch_SSL, best_blue_SSL))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cur epoch:0, cur blue:0.42918, best epoch:0, best blue:0.42918\n",
            "cur epoch:1, cur blue:0.42820, best epoch:0, best blue:0.42918\n",
            "cur epoch:2, cur blue:0.42781, best epoch:0, best blue:0.42918\n",
            "cur epoch:3, cur blue:0.43212, best epoch:3, best blue:0.43212\n",
            "cur epoch:4, cur blue:0.42866, best epoch:3, best blue:0.43212\n",
            "cur epoch:5, cur blue:0.41703, best epoch:3, best blue:0.43212\n",
            "cur epoch:6, cur blue:0.41722, best epoch:3, best blue:0.43212\n",
            "cur epoch:7, cur blue:0.41193, best epoch:3, best blue:0.43212\n",
            "cur epoch:8, cur blue:0.41364, best epoch:3, best blue:0.43212\n",
            "cur epoch:9, cur blue:0.40633, best epoch:3, best blue:0.43212\n",
            "cur epoch:10, cur blue:0.40415, best epoch:3, best blue:0.43212\n",
            "cur epoch:11, cur blue:0.40286, best epoch:3, best blue:0.43212\n",
            "cur epoch:12, cur blue:0.40308, best epoch:3, best blue:0.43212\n",
            "cur epoch:13, cur blue:0.40153, best epoch:3, best blue:0.43212\n",
            "cur epoch:14, cur blue:0.39194, best epoch:3, best blue:0.43212\n",
            "cur epoch:15, cur blue:0.39894, best epoch:3, best blue:0.43212\n",
            "cur epoch:16, cur blue:0.39166, best epoch:3, best blue:0.43212\n",
            "cur epoch:17, cur blue:0.39504, best epoch:3, best blue:0.43212\n",
            "cur epoch:18, cur blue:0.37896, best epoch:3, best blue:0.43212\n",
            "cur epoch:19, cur blue:0.41726, best epoch:3, best blue:0.43212\n",
            "cur epoch:20, cur blue:0.41450, best epoch:3, best blue:0.43212\n",
            "cur epoch:21, cur blue:0.40683, best epoch:3, best blue:0.43212\n",
            "cur epoch:22, cur blue:0.40812, best epoch:3, best blue:0.43212\n",
            "cur epoch:23, cur blue:0.41116, best epoch:3, best blue:0.43212\n",
            "cur epoch:24, cur blue:0.39474, best epoch:3, best blue:0.43212\n",
            "cur epoch:25, cur blue:0.42131, best epoch:3, best blue:0.43212\n",
            "cur epoch:26, cur blue:0.40487, best epoch:3, best blue:0.43212\n",
            "cur epoch:27, cur blue:0.41378, best epoch:3, best blue:0.43212\n",
            "cur epoch:28, cur blue:0.41809, best epoch:3, best blue:0.43212\n",
            "cur epoch:29, cur blue:0.41651, best epoch:3, best blue:0.43212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1ibr7Wety83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c36ae478-bd87-4969-e433-2bff7acaa047"
      },
      "source": [
        "model.load_state_dict(torch.load('/content/best_chatbot_models_state_dict_SSL.pth'))\n",
        "ece_criterion = _ECELoss().to(device)\n",
        "bleu, logits_all, labels_all = evaluation(model, test_loader)\n",
        "logits_all = logits_all.view(-1,ntokens)\n",
        "labels_all = labels_all.view(-1)\n",
        "temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
        "\n",
        "evaluate_matrics(model,test_loader)\n",
        "temperature_ece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU_SCORE1: 0.432117198665572 Precision: 0.5013206707205613 Recall: 0.47854662319421903 F1_Score: 0.48636758259571017 Meteor: 0.4218850844127235 PPL: 8.060325197033741\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.37642908096313477"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMPDaCdIIICe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWTCIMGP4mt2"
      },
      "source": [
        "## TS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0makdBS74n8w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}