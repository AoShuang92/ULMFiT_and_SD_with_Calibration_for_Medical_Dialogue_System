{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_GloVe_ULMFiT_TS.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AoShuang92/ULMFiT_and_SD_with_Calibration_for_Medical_Dialogue_System/blob/main/Transformer_GloVe_ULMFiT_TS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7-Yal7-o0vN",
        "outputId": "88d658a2-1b52-4bac-a4ba-2e4ba69ed6a5"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "#system\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from collections import Counter\n",
        "import json\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext import data\n",
        "\n",
        "def seed_everything(seed=27):\n",
        "  #random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYu0ifDf4qwv",
        "outputId": "20e00707-421f-402c-b5a7-14904d9d0468"
      },
      "source": [
        "!pip install \"nltk==3.4.5\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\r\u001b[K     |▎                               | 10kB 24.7MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 17.1MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 14.5MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 13.5MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 8.8MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 9.3MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 9.6MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81kB 10.6MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 9.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102kB 8.5MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112kB 8.5MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122kB 8.5MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 8.5MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143kB 8.5MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153kB 8.5MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163kB 8.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174kB 8.5MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 8.5MB/s eta 0:00:01\r\u001b[K     |████▎                           | 194kB 8.5MB/s eta 0:00:01\r\u001b[K     |████▌                           | 204kB 8.5MB/s eta 0:00:01\r\u001b[K     |████▊                           | 215kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 225kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 235kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 245kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 256kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 266kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 276kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 286kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 296kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 307kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 317kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 327kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 337kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 348kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 358kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 368kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 378kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 389kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 399kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 409kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 419kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 430kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 440kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 450kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 460kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 471kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 481kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 491kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 501kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 512kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 522kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 532kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 542kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 552kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 563kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 573kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 583kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 593kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 604kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 614kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 624kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 634kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 645kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 655kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 665kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 675kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 686kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 696kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 706kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 716kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 727kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 737kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 747kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 757kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 768kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 778kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 788kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 798kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 808kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 819kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 829kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 839kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 849kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 860kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 870kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 880kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 890kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 901kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 911kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 921kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 931kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 942kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 952kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 962kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 972kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 983kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 993kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.0MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.0MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.0MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.0MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.1MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.1MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.2MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.3MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.3MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.3MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.3MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.3MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.3MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.3MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.4MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.4MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.4MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.4MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.4MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5) (1.15.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449908 sha256=0de3b19122b21e97cebcaa538ef54619aa4b7a8985c01bb176cc1a948fed368e\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFHqt1eepjjr"
      },
      "source": [
        "max_length = 35\n",
        "train_dir = \"/content/drive/MyDrive/chatbot/combined_qa_train_ID.csv\"\n",
        "test_dir = \"/content/drive/MyDrive/chatbot/combined_qa_test_200_ID.csv\"\n",
        "#train_dir = \"/content/combined_qa_train_ID.csv\"\n",
        "#test_dir = \"/content/combined_qa_test_50_ID.csv\"\n",
        "batch_size = 4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def remove_unnecessary(text):\n",
        "    #remove_URL\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    text = url.sub('', text)\n",
        "\n",
        "    #remove_html\n",
        "    html = re.compile(r'<.*?>')\n",
        "    text = html.sub('', text)\n",
        "\n",
        "    #remove @\n",
        "    text = re.sub('@[^\\s]+','',text)\n",
        "\n",
        "    #remove_emoji\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    #Removes integers \n",
        "    text = ''.join([i for i in text if not i.isdigit()])         \n",
        "    \n",
        "    #remove_punct\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(table)\n",
        "\n",
        "    #Replaces contractions from a string to their equivalents \n",
        "    contraction_patterns = [(r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), \n",
        "                            (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
        "                            (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'),\n",
        "                            (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), \n",
        "                            (r'dont', 'do not'), (r'wont', 'will not')]\n",
        "    \n",
        "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
        "    for (pattern, repl) in patterns:\n",
        "        text, _= re.subn(pattern, repl, text)\n",
        "\n",
        "    #lemmatize_sentence\n",
        "    sentence_words = text.split(' ')\n",
        "    new_sentence_words = list()\n",
        "    \n",
        "    for sentence_word in sentence_words:\n",
        "        sentence_word = sentence_word.replace('#', '')\n",
        "        new_sentence_word = WordNetLemmatizer().lemmatize(sentence_word.lower(), wordnet.VERB)\n",
        "        new_sentence_words.append(new_sentence_word)\n",
        "        \n",
        "    new_sentence = ' '.join(new_sentence_words)\n",
        "    new_sentence = new_sentence.strip()\n",
        "\n",
        "    return new_sentence.lower()\n",
        "\n",
        "def prepare_csv(train,test):\n",
        "    # idx = np.arange(df_train.shape[0])    \n",
        "    # np.random.shuffle(idx)\n",
        "    # val_size = int(len(idx) * val_ratio)\n",
        "    if not os.path.exists('cache'): # cache is tem memory file \n",
        "        os.makedirs('cache')\n",
        "    \n",
        "    train_temp = train[['Question', 'Answer']].to_csv(\n",
        "        'cache/dataset_train.csv', index=True)\n",
        "    \n",
        "    test_temp = test[['Question', 'Answer']].to_csv(\n",
        "        'cache/dataset_val.csv', index=True) \n",
        "    return  train_temp,  test_temp\n",
        "\n",
        "def get_iterator(dataset, batch_size, train=True,\n",
        "                 shuffle=True, repeat=False, device=None): \n",
        "    dataset_iter = data.Iterator(\n",
        "        dataset, batch_size=batch_size, device=device,\n",
        "        train=train, shuffle=shuffle, repeat=repeat,\n",
        "        sort=False)  \n",
        "    return dataset_iter\n",
        "\n",
        "def get_dataset(fix_length=max_length, lower=False, vectors=None,train_dir = train_dir, test_dir = test_dir, batch_size=batch_size, device=None): \n",
        "    train = pd.read_csv(train_dir,error_bad_lines=False)\n",
        "    test =  pd.read_csv(test_dir,error_bad_lines=False)\n",
        "    train['Question'] = train['Question'].apply(lambda x: remove_unnecessary(x))\n",
        "    train['Answer'] = train['Answer'].apply(lambda x: remove_unnecessary(x))\n",
        "    test['Question'] = test['Question'].apply(lambda x: remove_unnecessary(x))\n",
        "    test['Answer'] = test['Answer'].apply(lambda x: remove_unnecessary(x))\n",
        "    train_temp,  test_temp = prepare_csv(train,test)\n",
        "    if vectors is not None:\n",
        "        lower=True\n",
        "\n",
        "    TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"spacy\"),init_token='<sos>',eos_token='<eos>',lower=True,batch_first=True, \n",
        "                      fix_length=fix_length)\n",
        "    ID = data.Field(use_vocab=False, sequential=False, dtype=torch.float16)  \n",
        "    train_temps = data.TabularDataset(\n",
        "        path='/content/cache/dataset_train.csv', format='csv', skip_header=True,\n",
        "        fields=[(\"ID\",ID),('Question', TEXT), ('Answer', TEXT)]) \n",
        "    test_temps = data.TabularDataset(\n",
        "        path='/content/cache/dataset_val.csv', format='csv', skip_header=True,\n",
        "        fields=[(\"ID\",ID),('Question', TEXT), ('Answer', TEXT)]) \n",
        "\n",
        "    TEXT.build_vocab(train_temps,test_temps, vectors=GloVe(name='6B', dim=300))\n",
        "    ID.build_vocab(train_temps, test_temps)\n",
        "    word_embeddings = TEXT.vocab.vectors\n",
        "    vocab_size = len(TEXT.vocab)\n",
        "    ntoken = len(TEXT.vocab.stoi)\n",
        "    print(\"vocab_size_and_ntokens:\",vocab_size,ntoken)\n",
        "    train_loader = get_iterator(train_temps, batch_size=batch_size, \n",
        "                                train=True, shuffle=True,\n",
        "                                repeat=False,device=None)\n",
        "    test_loader = get_iterator(test_temps, batch_size=batch_size, \n",
        "                            train=False, shuffle=False,\n",
        "                            repeat=False, device=None)\n",
        "    print('Train samples:%d'%(len(train_temps)), 'Valid samples:%d'%(len(test_temps)),'Train minibatch nb:%d'%(len(train_loader)),\n",
        "            'Valid minibatch nb:%d'%(len(test_loader)))\n",
        "    return vocab_size, word_embeddings, ntoken, train_loader, test_loader, TEXT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrQV-PKdqHOO",
        "outputId": "2fa8a75b-950d-4a50-ad16-0f9f8c770d21"
      },
      "source": [
        "vocab_size, word_embeddings, ntoken, train_loader, test_loader, TEXT = get_dataset(fix_length=max_length,train_dir = train_dir, test_dir = test_dir, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:26, 2.23MB/s]                          \n",
            "100%|█████████▉| 399413/400000 [00:37<00:00, 10343.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "vocab_size_and_ntokens: 1938 1938\n",
            "Train samples:1001 Valid samples:200 Train minibatch nb:251 Valid minibatch nb:50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJkEyCgwHRxQ",
        "outputId": "4164b964-a7a1-4cfd-e7e6-caf05d5b94d2"
      },
      "source": [
        "TEXT_ALL = torchtext.data.Field(tokenize=get_tokenizer(\"spacy\"),\n",
        "                            init_token='<sos>',\n",
        "                            eos_token='<eos>',\n",
        "                            lower=True)\n",
        "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT_ALL)\n",
        "TEXT_ALL.build_vocab(train_txt)\n",
        "ntokens = len(TEXT_ALL.vocab.stoi) \n",
        "print(ntokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading wikitext-2-v1.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "wikitext-2-v1.zip:   0%|          | 0.00/4.48M [00:00<?, ?B/s]\u001b[A\n",
            "wikitext-2-v1.zip:   1%|▏         | 65.5k/4.48M [00:00<00:08, 503kB/s]\u001b[A\n",
            "wikitext-2-v1.zip:   5%|▍         | 213k/4.48M [00:00<00:07, 602kB/s] \u001b[A\n",
            "wikitext-2-v1.zip:  21%|██        | 950k/4.48M [00:00<00:04, 822kB/s]\u001b[A\n",
            "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:00<00:00, 8.33MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting\n",
            "28871\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7PgYwYcqNP1"
      },
      "source": [
        "def create_masks(question, reply_input):\n",
        "    \n",
        "    def subsequent_mask(size):\n",
        "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "        return mask.unsqueeze(0)\n",
        "    \n",
        "    question_mask = question!=0\n",
        "    question_mask = question_mask.to(device)\n",
        "    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n",
        "     \n",
        "    reply_input_mask = reply_input!=0\n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n",
        "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n",
        "    \n",
        "    return question_mask, reply_input_mask\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(max_length, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len]\n",
        "    return data, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaD0bzWAsTme"
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements embeddings of the words and adds their positional encodings. \n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, max_len = max_length):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pe = self.create_positinal_encoding(max_len, self.d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def create_positinal_encoding(self, max_len, d_model):\n",
        "        pe = torch.zeros(max_len, d_model).to(device)\n",
        "        for pos in range(max_len):   # for each position of the word\n",
        "            for i in range(0, d_model, 2):   # for each dimension of the each position\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)   # include the batch size\n",
        "        return pe\n",
        "        \n",
        "    def forward(self, encoded_words):\n",
        "        embedding = self.embed(encoded_words) * math.sqrt(self.d_model)\n",
        "        #print(\"embedding\",embedding.size(),encoded_words.size())\n",
        "        #print(\"pe\",self.pe.size())\n",
        "        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n",
        "        embedding = self.dropout(embedding)\n",
        "        return embedding\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, heads, d_model):\n",
        "        \n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % heads == 0\n",
        "        self.d_k = d_model // heads\n",
        "        self.heads = heads\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        self.concat = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        query, key, value of shape: (batch_size, max_len, 512)\n",
        "        mask of shape: (batch_size, 1, 1, max_words)\n",
        "        \"\"\"\n",
        "        # (batch_size, max_len, 512)\n",
        "        query = self.query(query)\n",
        "        key = self.key(key)        \n",
        "        value = self.value(value)   \n",
        "        \n",
        "        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
        "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        \n",
        "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
        "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
        "        #scores = torch.matmul(query, key.permute(2,1,0,0)) / math.sqrt(query.size(-1))\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n",
        "        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n",
        "        weights = self.dropout(weights)\n",
        "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        context = torch.matmul(weights, value)\n",
        "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n",
        "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
        "        # (batch_size, max_len, h * d_k)\n",
        "        interacted = self.concat(context)\n",
        "        return interacted\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, middle_dim = 2048):\n",
        "        super(FeedForward, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
        "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.fc1(x))\n",
        "        out = self.fc2(self.dropout(out))\n",
        "        return out\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, heads):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, embeddings, mask):\n",
        "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
        "        interacted = self.layernorm(interacted + embeddings)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        encoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return encoded\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, heads):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
        "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
        "        query = self.layernorm(query + embeddings)\n",
        "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
        "        interacted = self.layernorm(interacted + query)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        decoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return decoded\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):    \n",
        "    def __init__(self, d_model, heads, num_layers, ntokens):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = ntokens\n",
        "        self.embed = Embeddings(self.vocab_size, d_model)#max_len\n",
        "        self.embed_dec = Embeddings(self.vocab_size, d_model,max_length)\n",
        "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        self.logit = nn.Linear(d_model, self.vocab_size)\n",
        "        self.temperature = nn.Parameter(torch.ones(1) * 1) \n",
        "        \n",
        "    def encode(self, src_words, src_mask):\n",
        "        src_embeddings = self.embed(src_words)\n",
        "        for layer in self.encoder:\n",
        "            src_embeddings = layer(src_embeddings, src_mask)\n",
        "        return src_embeddings\n",
        "    \n",
        "    def decode(self, target_words, target_mask, src_embeddings, src_mask):\n",
        "        tgt_embeddings = self.embed_dec(target_words)\n",
        "        for layer in self.decoder:\n",
        "            tgt_embeddings = layer(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
        "        return tgt_embeddings\n",
        "        \n",
        "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
        "        encoded = self.encode(src_words, src_mask)\n",
        "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
        "        out = F.log_softmax(self.logit(decoded) / self.temperature, dim = 2)\n",
        "        return out\n",
        "\n",
        "class AdamWarmup:\n",
        "    \n",
        "    def __init__(self, model_size, warmup_steps, optimizer):\n",
        "        \n",
        "        self.model_size = model_size\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.optimizer = optimizer\n",
        "        self.current_step = 0\n",
        "        self.lr = 0\n",
        "        \n",
        "    def get_lr(self):\n",
        "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
        "        \n",
        "    def step(self):\n",
        "        # Increment the number of steps each time we call the step function\n",
        "        self.current_step += 1\n",
        "        lr = self.get_lr()\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        # update the learning rate\n",
        "        self.lr = lr\n",
        "        self.optimizer.step()\n",
        "\n",
        "class LossWithLS(nn.Module):\n",
        "\n",
        "    def __init__(self, size, smooth):\n",
        "        super(LossWithLS, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n",
        "        self.confidence = 1.0 - smooth\n",
        "        self.smooth = smooth\n",
        "        self.size = size\n",
        "        \n",
        "    def forward(self, prediction, target, mask):\n",
        "        \"\"\"\n",
        "        prediction of shape: (batch_size, max_words, vocab_size)\n",
        "        target and mask of shape: (batch_size, max_words)\n",
        "        \"\"\"\n",
        "        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n",
        "        target = target.contiguous().view(-1)   # (batch_size * max_words)\n",
        "        mask = mask.float()\n",
        "        mask = mask.view(-1)       # (batch_size * max_words)\n",
        "        labels = prediction.data.clone()\n",
        "        labels.fill_(self.smooth / (self.size - 1))\n",
        "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n",
        "        loss = (loss.sum(1) * mask).sum() / mask.sum()\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qbLMn34sYmS"
      },
      "source": [
        "def train(train_loader, transformer, criterion, epoch):    \n",
        "    transformer.train()\n",
        "    sum_loss = 0\n",
        "    count = 0\n",
        "    \n",
        "    for i, pair in enumerate(train_loader): \n",
        "    #for i, (question, reply) in enumerate(train_loader):     \n",
        "        #samples = question.shape[0]\n",
        "        # Move to device\n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        \n",
        "        # Prepare Target Data\n",
        "        #print(\"pair\",pair, type(pair),len(pair))\n",
        "        #print(\"reply\",type(reply))\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "\n",
        "        # Create mask and add dimensions\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        reply_target = reply_target.reshape(-1)\n",
        "        loss = criterion(out.view(-1, ntokens), reply_target)\n",
        "        #loss = criterion(out, reply_target)\n",
        "        \n",
        "        # Backprop\n",
        "        transformer_optimizer.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        transformer_optimizer.step()\n",
        "        \n",
        "        #sum_loss += loss.item() * samples\n",
        "        #count += samples\n",
        "        #break\n",
        "        # if i % 100 == 0:\n",
        "        #     print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))\n",
        "\n",
        "def valid (test_loader,transformer): \n",
        "    all_blue = []\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    #rev_word_map = {v: k for k, v in word_embeddings.items()}\n",
        "    transformer.eval()\n",
        "    for i, pair in enumerate(test_loader):\n",
        "    #for i, (question, reply) in enumerate(test_loader):\n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        #loss = criterion(out, reply_target, reply_target_mask)\n",
        "        _, next = torch.max(out, dim = 2)# 2x51\n",
        "        for idx in range(next.shape[0]):\n",
        "            pred_sentence= prediction_ids2sentence(next[idx]).split()\n",
        "            gt=prediction_ids2sentence(reply_target[idx]).split()\n",
        "            BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "            # BLEU_2 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 0, 0))\n",
        "            # BLEU_3 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 0))\n",
        "            # BLEU_4 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 1))\n",
        "            all_blue.append(BLEU_1)\n",
        "    #print(\"BLEU_score:\",np.mean(all_blue))\n",
        "    return np.mean(all_blue)\n",
        "\n",
        "def evaluate(transformer, question, question_mask, max_len):\n",
        "    \"\"\"\n",
        "    Performs Greedy Decoding with a batch size of 1\n",
        "    \"\"\"\n",
        "    #rev_word_map = {v: k for k, v in word_embeddings.items()}\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    transformer.eval()\n",
        "    start_token = word_map['<sos>']\n",
        "    encoded = transformer.encode(question, question_mask)\n",
        "    words = torch.LongTensor([[start_token]]).to(device)\n",
        "    next_word = -22\n",
        "    while next_word != word_map['<eos>']:\n",
        "    #for step in range(max_len - 1):\n",
        "        size = words.shape[1]\n",
        "        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
        "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
        "        predictions = transformer.logit(decoded[:, -1])\n",
        "        _, next_word = torch.max(predictions, dim = 1)\n",
        "        next_word = next_word.item()\n",
        "        if next_word == word_map['<eos>'] or words.shape[1]==(max_len+1):\n",
        "            break\n",
        "        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n",
        "        \n",
        "    # Construct Sentence\n",
        "    if words.dim() == 2:\n",
        "        words = words.squeeze(0)\n",
        "        words = words.tolist()\n",
        "        \n",
        "    sen_idx = [w for w in words if w not in {word_map['<sos>']}]\n",
        "    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n",
        "    \n",
        "\n",
        "    return sentence\n",
        "\n",
        "def prediction_ids2sentence(pred_ids):\n",
        "    #rev_word_map = {v: k for k, v in word_embeddings.items()}\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    #_, next = torch.max(out, dim = 2)\n",
        "    sen_idx = []\n",
        "    for w in pred_ids:\n",
        "        if w == word_map['<eos>']:\n",
        "            break\n",
        "        sen_idx.append(w)\n",
        "    #print(sen_idx)\n",
        "    sentence = ' '.join([rev_word_map[int(sen_idx[k])] for k in range(len(sen_idx))])\n",
        "    return sentence\n",
        "\n",
        "from nltk.metrics import accuracy, precision, recall, f_measure\n",
        "\n",
        "def evaluate_matrics(transformer,test_loader):\n",
        "    sum_loss = 0\n",
        "    all_blue1 = []\n",
        "    all_blue2 = []\n",
        "    all_blue3 = []\n",
        "    all_blue4 = []\n",
        "\n",
        "    all_acc = []\n",
        "    all_prec = []\n",
        "    all_rec = []\n",
        "    all_f_score = []\n",
        "    all_meteor = []\n",
        "    word_map = TEXT.vocab.stoi\n",
        "    rev_word_map = TEXT.vocab.itos\n",
        "    #rev_word_map = {v: k for k, v in word_map_all.items()}\n",
        "    transformer.eval()\n",
        "    for i, pair in enumerate(test_loader):\n",
        "        #print(i)\n",
        "    \n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        \n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "        reply_target_mask = reply_target.reshape(-1)\n",
        "        loss = criterion(out.view(-1, ntokens), reply_target_mask)\n",
        "        #loss = criterion(out, reply_target, reply_target_mask)\n",
        "        sum_loss += loss.item()\n",
        "        #loss = criterion(out, reply_target, reply_target_mask)\n",
        "        _, next = torch.max(out, dim = 2)# 2x51\n",
        "        #print(\"next\",next.size(),\"next0\",next[0].size(),\"next1\",next[1].size())\n",
        "        for idx in range(next.shape[0]):\n",
        "            pred_sentence= prediction_ids2sentence(next[idx]).split()\n",
        "            \n",
        "            gt=prediction_ids2sentence(reply_target[idx]).split()\n",
        "            BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "            #BLEU_2 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 0, 0))\n",
        "            #BLEU_3 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 0))\n",
        "            #BLEU_4 = sentence_bleu(([gt]), pred_sentence, weights=(1, 1, 1, 1))\n",
        "            #print (type(gt),type(pred_sentence))\n",
        "            reference_set = set(gt)\n",
        "            test_set = set(pred_sentence)\n",
        "            prec = precision(reference_set, test_set)\n",
        "            rec = recall(reference_set, test_set)\n",
        "            f_score = f_measure(reference_set, test_set)\n",
        "            meteor = single_meteor_score( str(gt), str(pred_sentence))\n",
        "            all_blue1.append(BLEU_1)\n",
        "            #all_blue2.append(BLEU_2)\n",
        "            #all_blue3.append(BLEU_3)\n",
        "            #all_blue4.append(BLEU_4)\n",
        "            all_prec.append(prec)\n",
        "            all_rec.append(rec)\n",
        "            all_f_score.append(f_score)\n",
        "            all_meteor.append(meteor)\n",
        "            #\"Recall:\",np.mean(all_rec),\n",
        "    pre = np.mean(all_prec)\n",
        "    recall_score = np.mean(all_rec)\n",
        "    f1 = np.mean(all_f_score)\n",
        "    met = np.mean(all_meteor)\n",
        "    ppl = math.exp(sum_loss/i)\n",
        "\n",
        "    print(\"BLEU_SCORE1:\",np.mean(all_blue1), \"Precision:\",np.mean(all_prec), \"Recall:\",np.mean(all_rec),\"F1_Score:\",np.mean(all_f_score), \"Meteor:\",np.mean(all_meteor),\"PPL:\",math.exp(sum_loss/i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqP7isLu9FBo"
      },
      "source": [
        "class _ECELoss(nn.Module):\n",
        "    def __init__(self, n_bins=15):\n",
        "        \"\"\"\n",
        "        n_bins (int): number of confidence interval bins\n",
        "        \"\"\"\n",
        "        super(_ECELoss, self).__init__()\n",
        "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "        self.bin_lowers = bin_boundaries[:-1]\n",
        "        self.bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        softmaxes = F.softmax(logits, dim=1)\n",
        "        confidences, predictions = torch.max(softmaxes, 1)\n",
        "        accuracies = predictions.eq(labels)\n",
        "        ece = torch.zeros(1, device=logits.device)\n",
        "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
        "            # Calculated |confidence - accuracy| in each bin\n",
        "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
        "            prop_in_bin = in_bin.float().mean()\n",
        "            if prop_in_bin.item() > 0:\n",
        "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "        return ece\n",
        "\n",
        "def evaluation(model, test_loader):\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    logits_list = []\n",
        "    labels_list = []\n",
        "    all_blue = []\n",
        "    all_acc = []\n",
        "    all_rec = []\n",
        "    all_f_score = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        #for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        for i, pair in enumerate(test_loader):\n",
        "    \n",
        "            inputs = pair.Question\n",
        "            targets = pair.Answer\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            targets = targets[:, 1:]\n",
        "            input_mask, targets_mask = create_masks(inputs, targets)\n",
        "            outputs = model(inputs, input_mask, targets, targets_mask)\n",
        "            #print(outputs.shape)\n",
        "            logits_list.append(outputs)\n",
        "            labels_list.append(targets)\n",
        "            # target_loss = targets.reshape(-1)\n",
        "            # loss = criterion(outputs.view(-1, ntokens), target_loss)\n",
        "            # test_loss += loss.item()\n",
        "            _, predicted = outputs.max(2)\n",
        "        \n",
        "            for idx in range(predicted.shape[0]):\n",
        "                \n",
        "                pred_sentence= prediction_ids2sentence(predicted[idx]).split()\n",
        "                gt=prediction_ids2sentence(targets[idx]).split()\n",
        "                BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "                reference_set = set(gt)\n",
        "                test_set = set(pred_sentence)\n",
        "                #rec = recall(reference_set, test_set)\n",
        "                #f_score = f_measure(reference_set, test_set)\n",
        "                all_blue.append(BLEU_1)\n",
        "                #all_rec.append(rec)\n",
        "                #all_f_score.append(f_score)\n",
        "\n",
        "    logits_all = torch.cat(logits_list).cuda()\n",
        "    labels_all = torch.cat(labels_list).cuda()\n",
        "    \n",
        "    return np.mean(all_blue),logits_all, labels_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyNgTQMRse52"
      },
      "source": [
        "seed_everything()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens)\n",
        "model = model.to(device)\n",
        "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "\n",
        "best_blue = 0\n",
        "best_epoch = 0\n",
        "for epoch in range(30):\n",
        "    train(train_loader, model, criterion, epoch)\n",
        "    blue_score = valid (test_loader,model)\n",
        "    if blue_score > best_blue:\n",
        "        best_blue = blue_score\n",
        "        best_epoch = epoch\n",
        "        state = {'epoch': epoch, 'transformer': model, 'transformer_optimizer': transformer_optimizer}\n",
        "        torch.save(state, 'best_chatbot_model_object.pth.tar')\n",
        "        torch.save(model.state_dict(), 'best_chatbot_models_state_dict.pth')\n",
        "    print('cur epoch:%d, cur blue:%.5f, best epoch:%d, best blue:%.5f'%(epoch,blue_score, best_epoch, best_blue))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfWg-zh7ssgL",
        "outputId": "9b4d80a1-c335-4efa-d7e6-a79748a898b8"
      },
      "source": [
        "seed_everything()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens)\n",
        "model = model.to(device)\n",
        "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_LM_wiki_model.pth'),strict=False)\n",
        "model = model.to(device)\n",
        "best_blue_SSL = 0\n",
        "best_epoch_SSL = 0\n",
        "for epoch_SSL in range(100):\n",
        "    train(train_loader, model, criterion, epoch_SSL)\n",
        "    blue_score_SSL = valid (test_loader,model)\n",
        "    if blue_score_SSL > best_blue_SSL:\n",
        "        best_blue_SSL = blue_score_SSL\n",
        "        best_epoch_SSL = epoch_SSL\n",
        "        state = {'epoch': epoch_SSL, 'transformer': model, 'transformer_optimizer': transformer_optimizer}\n",
        "        torch.save(state, '/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_model_object.pth.tar')\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_ulmfit_state_dict.pth')\n",
        "    print('cur epoch:%d, cur blue:%.5f, best epoch:%d, best blue:%.5f'%(epoch_SSL,blue_score_SSL, best_epoch_SSL, best_blue_SSL))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cur epoch:0, cur blue:0.01992, best epoch:0, best blue:0.01992\n",
            "cur epoch:1, cur blue:0.01432, best epoch:0, best blue:0.01992\n",
            "cur epoch:2, cur blue:0.04555, best epoch:2, best blue:0.04555\n",
            "cur epoch:3, cur blue:0.05122, best epoch:3, best blue:0.05122\n",
            "cur epoch:4, cur blue:0.03790, best epoch:3, best blue:0.05122\n",
            "cur epoch:5, cur blue:0.05372, best epoch:5, best blue:0.05372\n",
            "cur epoch:6, cur blue:0.05261, best epoch:5, best blue:0.05372\n",
            "cur epoch:7, cur blue:0.09705, best epoch:7, best blue:0.09705\n",
            "cur epoch:8, cur blue:0.10207, best epoch:8, best blue:0.10207\n",
            "cur epoch:9, cur blue:0.11002, best epoch:9, best blue:0.11002\n",
            "cur epoch:10, cur blue:0.10470, best epoch:9, best blue:0.11002\n",
            "cur epoch:11, cur blue:0.11275, best epoch:11, best blue:0.11275\n",
            "cur epoch:12, cur blue:0.13032, best epoch:12, best blue:0.13032\n",
            "cur epoch:13, cur blue:0.12860, best epoch:12, best blue:0.13032\n",
            "cur epoch:14, cur blue:0.12234, best epoch:12, best blue:0.13032\n",
            "cur epoch:15, cur blue:0.12238, best epoch:12, best blue:0.13032\n",
            "cur epoch:16, cur blue:0.08363, best epoch:12, best blue:0.13032\n",
            "cur epoch:17, cur blue:0.12687, best epoch:12, best blue:0.13032\n",
            "cur epoch:18, cur blue:0.13302, best epoch:18, best blue:0.13302\n",
            "cur epoch:19, cur blue:0.14738, best epoch:19, best blue:0.14738\n",
            "cur epoch:20, cur blue:0.13805, best epoch:19, best blue:0.14738\n",
            "cur epoch:21, cur blue:0.15111, best epoch:21, best blue:0.15111\n",
            "cur epoch:22, cur blue:0.16108, best epoch:22, best blue:0.16108\n",
            "cur epoch:23, cur blue:0.16252, best epoch:23, best blue:0.16252\n",
            "cur epoch:24, cur blue:0.14353, best epoch:23, best blue:0.16252\n",
            "cur epoch:25, cur blue:0.16243, best epoch:23, best blue:0.16252\n",
            "cur epoch:26, cur blue:0.17048, best epoch:26, best blue:0.17048\n",
            "cur epoch:27, cur blue:0.17776, best epoch:27, best blue:0.17776\n",
            "cur epoch:28, cur blue:0.17289, best epoch:27, best blue:0.17776\n",
            "cur epoch:29, cur blue:0.18524, best epoch:29, best blue:0.18524\n",
            "cur epoch:30, cur blue:0.16647, best epoch:29, best blue:0.18524\n",
            "cur epoch:31, cur blue:0.18063, best epoch:29, best blue:0.18524\n",
            "cur epoch:32, cur blue:0.19768, best epoch:32, best blue:0.19768\n",
            "cur epoch:33, cur blue:0.18905, best epoch:32, best blue:0.19768\n",
            "cur epoch:34, cur blue:0.19646, best epoch:32, best blue:0.19768\n",
            "cur epoch:35, cur blue:0.19960, best epoch:35, best blue:0.19960\n",
            "cur epoch:36, cur blue:0.20345, best epoch:36, best blue:0.20345\n",
            "cur epoch:37, cur blue:0.22360, best epoch:37, best blue:0.22360\n",
            "cur epoch:38, cur blue:0.19185, best epoch:37, best blue:0.22360\n",
            "cur epoch:39, cur blue:0.22671, best epoch:39, best blue:0.22671\n",
            "cur epoch:40, cur blue:0.22906, best epoch:40, best blue:0.22906\n",
            "cur epoch:41, cur blue:0.19422, best epoch:40, best blue:0.22906\n",
            "cur epoch:42, cur blue:0.22916, best epoch:42, best blue:0.22916\n",
            "cur epoch:43, cur blue:0.20279, best epoch:42, best blue:0.22916\n",
            "cur epoch:44, cur blue:0.23560, best epoch:44, best blue:0.23560\n",
            "cur epoch:45, cur blue:0.23218, best epoch:44, best blue:0.23560\n",
            "cur epoch:46, cur blue:0.22625, best epoch:44, best blue:0.23560\n",
            "cur epoch:47, cur blue:0.24221, best epoch:47, best blue:0.24221\n",
            "cur epoch:48, cur blue:0.24709, best epoch:48, best blue:0.24709\n",
            "cur epoch:49, cur blue:0.24633, best epoch:48, best blue:0.24709\n",
            "cur epoch:50, cur blue:0.24759, best epoch:50, best blue:0.24759\n",
            "cur epoch:51, cur blue:0.27074, best epoch:51, best blue:0.27074\n",
            "cur epoch:52, cur blue:0.25000, best epoch:51, best blue:0.27074\n",
            "cur epoch:53, cur blue:0.24977, best epoch:51, best blue:0.27074\n",
            "cur epoch:54, cur blue:0.25955, best epoch:51, best blue:0.27074\n",
            "cur epoch:55, cur blue:0.25129, best epoch:51, best blue:0.27074\n",
            "cur epoch:56, cur blue:0.26444, best epoch:51, best blue:0.27074\n",
            "cur epoch:57, cur blue:0.27278, best epoch:57, best blue:0.27278\n",
            "cur epoch:58, cur blue:0.27376, best epoch:58, best blue:0.27376\n",
            "cur epoch:59, cur blue:0.27135, best epoch:58, best blue:0.27376\n",
            "cur epoch:60, cur blue:0.28952, best epoch:60, best blue:0.28952\n",
            "cur epoch:61, cur blue:0.26403, best epoch:60, best blue:0.28952\n",
            "cur epoch:62, cur blue:0.25602, best epoch:60, best blue:0.28952\n",
            "cur epoch:63, cur blue:0.27230, best epoch:60, best blue:0.28952\n",
            "cur epoch:64, cur blue:0.28353, best epoch:60, best blue:0.28952\n",
            "cur epoch:65, cur blue:0.27959, best epoch:60, best blue:0.28952\n",
            "cur epoch:66, cur blue:0.27603, best epoch:60, best blue:0.28952\n",
            "cur epoch:67, cur blue:0.28664, best epoch:60, best blue:0.28952\n",
            "cur epoch:68, cur blue:0.28837, best epoch:60, best blue:0.28952\n",
            "cur epoch:69, cur blue:0.28246, best epoch:60, best blue:0.28952\n",
            "cur epoch:70, cur blue:0.29902, best epoch:70, best blue:0.29902\n",
            "cur epoch:71, cur blue:0.28376, best epoch:70, best blue:0.29902\n",
            "cur epoch:72, cur blue:0.29199, best epoch:70, best blue:0.29902\n",
            "cur epoch:73, cur blue:0.30453, best epoch:73, best blue:0.30453\n",
            "cur epoch:74, cur blue:0.30931, best epoch:74, best blue:0.30931\n",
            "cur epoch:75, cur blue:0.29494, best epoch:74, best blue:0.30931\n",
            "cur epoch:76, cur blue:0.29958, best epoch:74, best blue:0.30931\n",
            "cur epoch:77, cur blue:0.30576, best epoch:74, best blue:0.30931\n",
            "cur epoch:78, cur blue:0.31015, best epoch:78, best blue:0.31015\n",
            "cur epoch:79, cur blue:0.29845, best epoch:78, best blue:0.31015\n",
            "cur epoch:80, cur blue:0.31600, best epoch:80, best blue:0.31600\n",
            "cur epoch:81, cur blue:0.30765, best epoch:80, best blue:0.31600\n",
            "cur epoch:82, cur blue:0.32572, best epoch:82, best blue:0.32572\n",
            "cur epoch:83, cur blue:0.30363, best epoch:82, best blue:0.32572\n",
            "cur epoch:84, cur blue:0.30141, best epoch:82, best blue:0.32572\n",
            "cur epoch:85, cur blue:0.30322, best epoch:82, best blue:0.32572\n",
            "cur epoch:86, cur blue:0.31283, best epoch:82, best blue:0.32572\n",
            "cur epoch:87, cur blue:0.29840, best epoch:82, best blue:0.32572\n",
            "cur epoch:88, cur blue:0.32457, best epoch:82, best blue:0.32572\n",
            "cur epoch:89, cur blue:0.31557, best epoch:82, best blue:0.32572\n",
            "cur epoch:90, cur blue:0.30358, best epoch:82, best blue:0.32572\n",
            "cur epoch:91, cur blue:0.32250, best epoch:82, best blue:0.32572\n",
            "cur epoch:92, cur blue:0.32527, best epoch:82, best blue:0.32572\n",
            "cur epoch:93, cur blue:0.34012, best epoch:93, best blue:0.34012\n",
            "cur epoch:94, cur blue:0.33009, best epoch:93, best blue:0.34012\n",
            "cur epoch:95, cur blue:0.35043, best epoch:95, best blue:0.35043\n",
            "cur epoch:96, cur blue:0.33500, best epoch:95, best blue:0.35043\n",
            "cur epoch:97, cur blue:0.33074, best epoch:95, best blue:0.35043\n",
            "cur epoch:98, cur blue:0.33071, best epoch:95, best blue:0.35043\n",
            "cur epoch:99, cur blue:0.35621, best epoch:99, best blue:0.35621\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-6PvEVj56s4",
        "outputId": "272f0a6d-4183-4abb-f944-f49556aded12"
      },
      "source": [
        "ece_criterion = _ECELoss().to(device)\n",
        "bleu, logits_all, labels_all = evaluation(model, test_loader)\n",
        "logits_all = logits_all.view(-1,ntokens)\n",
        "labels_all = labels_all.view(-1)\n",
        "temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
        "\n",
        "evaluate_matrics(model,test_loader)\n",
        "temperature_ece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1ibr7Wety83"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class ModelWithTemperature(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(ModelWithTemperature, self).__init__()\n",
        "        self.model = model\n",
        "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
        "\n",
        "    def forward(self, inputs, input_mask, targets, targets_mask):\n",
        "        logits = self.model(inputs, input_mask, targets, targets_mask)\n",
        "        return self.temperature_scale(logits)\n",
        "\n",
        "    def temperature_scale(self, logits):\n",
        "        # Expand temperature to match the size of logits\n",
        "        temperature = self.temperature.unsqueeze(1).expand(logits.size())\n",
        "        return logits / temperature\n",
        "\n",
        "    # This function probably should live outside of this class, but whatever\n",
        "    def set_temperature(self, valid_loader):\n",
        "        self.cuda()\n",
        "        ece_criterion = _ECELoss().cuda()\n",
        "        nll_criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "        # First: collect all the logits and labels for the validation set\n",
        "        logits_list = []\n",
        "        labels_list = []\n",
        "        with torch.no_grad():\n",
        "            for i, pair in enumerate(valid_loader):\n",
        "    \n",
        "                input = pair.Question.cuda()\n",
        "                label = pair.Answer.cuda()\n",
        "            #for input, label in valid_loader:\n",
        "                input = input.cuda()\n",
        "                label = label.cuda()\n",
        "                label = label[:, 1:]\n",
        "                input_mask, label_mask = create_masks(input, label)\n",
        "                logits = self.model(input, input_mask, label, label_mask)\n",
        "                logits_list.append(logits)\n",
        "                labels_list.append(label)\n",
        "            logits = torch.cat(logits_list).cuda()\n",
        "            labels = torch.cat(labels_list).cuda()\n",
        "            \n",
        "\n",
        "        # Next: optimize the temperature w.r.t. NLL\n",
        "        init_temp = self.temperature.clone()\n",
        "        optimizer = optim.LBFGS([self.temperature], lr=0.01, max_iter=50)\n",
        "\n",
        "        def eval():\n",
        "            labels_loss = labels.reshape(-1)\n",
        "            loss = nll_criterion(self.temperature_scale(logits.view(-1, ntokens)), labels_loss)\n",
        "            loss.backward()\n",
        "            return loss\n",
        "        optimizer.step(eval)\n",
        "\n",
        "        # CalculateECE after temperature scaling\n",
        "        labels_loss = labels.reshape(-1)\n",
        "        after_temperature_ece = ece_criterion(self.temperature_scale(logits.view(-1,ntokens )), labels_loss).item()\n",
        "        print('Initial temperature: %.3f, Optimal temperature: %.3f' % (init_temp, self.temperature.item()))\n",
        "        return self\n",
        "\n",
        "class _ECELoss(nn.Module):\n",
        "    def __init__(self, n_bins=15):\n",
        "        \"\"\"\n",
        "        n_bins (int): number of confidence interval bins\n",
        "        \"\"\"\n",
        "        super(_ECELoss, self).__init__()\n",
        "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "        self.bin_lowers = bin_boundaries[:-1]\n",
        "        self.bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        softmaxes = F.softmax(logits, dim=1)\n",
        "        confidences, predictions = torch.max(softmaxes, 1)\n",
        "        accuracies = predictions.eq(labels)\n",
        "        ece = torch.zeros(1, device=logits.device)\n",
        "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
        "            # Calculated |confidence - accuracy| in each bin\n",
        "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
        "            prop_in_bin = in_bin.float().mean()\n",
        "            if prop_in_bin.item() > 0:\n",
        "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "        return ece\n",
        "\n",
        "def evaluation(model, test_loader):\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    logits_list = []\n",
        "    labels_list = []\n",
        "    all_blue = []\n",
        "    all_acc = []\n",
        "    all_rec = []\n",
        "    all_f_score = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        #for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        for i, pair in enumerate(test_loader):\n",
        "    \n",
        "            inputs = pair.Question\n",
        "            targets = pair.Answer\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            targets = targets[:, 1:]\n",
        "            input_mask, targets_mask = create_masks(inputs, targets)\n",
        "            outputs = model(inputs, input_mask, targets, targets_mask)\n",
        "            logits_list.append(outputs)\n",
        "            labels_list.append(targets)\n",
        "            target_loss = targets.reshape(-1)\n",
        "            loss = criterion(outputs.view(-1, ntokens), target_loss)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(2)\n",
        "        \n",
        "            for idx in range(predicted.shape[0]):\n",
        "                \n",
        "                pred_sentence= prediction_ids2sentence(predicted[idx]).split()\n",
        "                gt=prediction_ids2sentence(targets[idx]).split()\n",
        "                BLEU_1 = sentence_bleu(([gt]), pred_sentence, weights=(1, 0, 0, 0))\n",
        "                reference_set = set(gt)\n",
        "                test_set = set(pred_sentence)\n",
        "                #rec = recall(reference_set, test_set)\n",
        "                #f_score = f_measure(reference_set, test_set)\n",
        "                all_blue.append(BLEU_1)\n",
        "                #all_rec.append(rec)\n",
        "                #all_f_score.append(f_score)\n",
        "    logits_all = torch.cat(logits_list).cuda()\n",
        "    labels_all = torch.cat(labels_list).cuda()\n",
        "    \n",
        "    return np.mean(all_blue),logits_all, labels_all\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZE115I_OeL7",
        "outputId": "b44c2932-fc71-4529-edc5-5473f64bc9f2"
      },
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_ulmfit_state_dict.pth'),strict=False)\n",
        "model.eval()\n",
        "ece_criterion = _ECELoss().to(device)\n",
        "bleu, logits_all, labels_all = evaluation(model, test_loader)\n",
        "logits_all = logits_all.view(-1,ntokens)\n",
        "labels_all = labels_all.view(-1)\n",
        "temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
        "print('Before TS- bleu:%.3f, bef ece:%.5f'%(bleu,temperature_ece))\n",
        "\n",
        "model_ts = ModelWithTemperature(model)\n",
        "model_ts.set_temperature(test_loader)\n",
        "bleu, logits_all, labels_all = evaluation(model_ts, test_loader)\n",
        "logits_all = logits_all.view(-1,ntokens)\n",
        "labels_all = labels_all.view(-1)\n",
        "temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
        "print('After TS- bleu:%.3f,aft ece:%.5f'%(bleu,temperature_ece))\n",
        "#torch.save(model_ts.state_dict(), 'best_model_ts.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Before TS- bleu:0.166, bef ece:0.32469\n",
            "Initial temperature: 1.500, Optimal temperature: 3.430\n",
            "After TS- bleu:0.166,aft ece:0.21836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx3-1S-vOnEn",
        "outputId": "740073aa-af0f-4b15-bb4e-a7370764dbfb"
      },
      "source": [
        "seed_everything()\n",
        "#model_loaded_ULMFIT_TS = model.load_state_dict(torch.load('best_model_ts.pth'), strict = False)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#checkpoint = torch.load('/content/best_chatbot_model_object.pth.tar')\n",
        "#model_loaded_ULMFIT_TS = checkpoint['transformer']\n",
        "#model = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map_all)\n",
        "#model_loaded_ULMFIT_TS = model_loaded_ULMFIT_TS.to(device)\n",
        "#adam_optimizer = torch.optim.Adam(model_loaded_ULMFIT_TS.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "#transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "\n",
        "best_blue = 0\n",
        "best_epoch = 0\n",
        "for epoch in range(100):\n",
        "    train(train_loader, model_ts, criterion, epoch)\n",
        "    blue_score = valid (test_loader,model_ts)\n",
        "    if blue_score > best_blue:\n",
        "        best_blue = blue_score\n",
        "        best_epoch = epoch\n",
        "        #state = {'epoch': epoch, 'transformer': model_loaded_ULMFIT_TS, 'transformer_optimizer': transformer_optimizer}\n",
        "        #torch.save(state, 'best_chatbot_model_object_ULMFIT_SSL_TS.pth.tar')\n",
        "        #torch.save(model_loaded_ULMFIT_TS.state_dict(), 'best_chatbot_models_state_dict_ULMFIT_SSL_TS.pth')\n",
        "    print('cur epoch:%d, cur blue:%.4f, best epoch:%d, best blue:%.4f'%(epoch,blue_score, best_epoch, best_blue))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cur epoch:0, cur blue:0.2208, best epoch:0, best blue:0.2208\n",
            "cur epoch:1, cur blue:0.2347, best epoch:1, best blue:0.2347\n",
            "cur epoch:2, cur blue:0.2371, best epoch:2, best blue:0.2371\n",
            "cur epoch:3, cur blue:0.2230, best epoch:2, best blue:0.2371\n",
            "cur epoch:4, cur blue:0.2122, best epoch:2, best blue:0.2371\n",
            "cur epoch:5, cur blue:0.2178, best epoch:2, best blue:0.2371\n",
            "cur epoch:6, cur blue:0.2374, best epoch:6, best blue:0.2374\n",
            "cur epoch:7, cur blue:0.2193, best epoch:6, best blue:0.2374\n",
            "cur epoch:8, cur blue:0.2201, best epoch:6, best blue:0.2374\n",
            "cur epoch:9, cur blue:0.2163, best epoch:6, best blue:0.2374\n",
            "cur epoch:10, cur blue:0.2249, best epoch:6, best blue:0.2374\n",
            "cur epoch:11, cur blue:0.2244, best epoch:6, best blue:0.2374\n",
            "cur epoch:12, cur blue:0.2423, best epoch:12, best blue:0.2423\n",
            "cur epoch:13, cur blue:0.2376, best epoch:12, best blue:0.2423\n",
            "cur epoch:14, cur blue:0.2239, best epoch:12, best blue:0.2423\n",
            "cur epoch:15, cur blue:0.2241, best epoch:12, best blue:0.2423\n",
            "cur epoch:16, cur blue:0.2237, best epoch:12, best blue:0.2423\n",
            "cur epoch:17, cur blue:0.2271, best epoch:12, best blue:0.2423\n",
            "cur epoch:18, cur blue:0.2224, best epoch:12, best blue:0.2423\n",
            "cur epoch:19, cur blue:0.2283, best epoch:12, best blue:0.2423\n",
            "cur epoch:20, cur blue:0.2239, best epoch:12, best blue:0.2423\n",
            "cur epoch:21, cur blue:0.2326, best epoch:12, best blue:0.2423\n",
            "cur epoch:22, cur blue:0.2357, best epoch:12, best blue:0.2423\n",
            "cur epoch:23, cur blue:0.2164, best epoch:12, best blue:0.2423\n",
            "cur epoch:24, cur blue:0.2081, best epoch:12, best blue:0.2423\n",
            "cur epoch:25, cur blue:0.2168, best epoch:12, best blue:0.2423\n",
            "cur epoch:26, cur blue:0.2276, best epoch:12, best blue:0.2423\n",
            "cur epoch:27, cur blue:0.2133, best epoch:12, best blue:0.2423\n",
            "cur epoch:28, cur blue:0.2138, best epoch:12, best blue:0.2423\n",
            "cur epoch:29, cur blue:0.2145, best epoch:12, best blue:0.2423\n",
            "cur epoch:30, cur blue:0.2108, best epoch:12, best blue:0.2423\n",
            "cur epoch:31, cur blue:0.2283, best epoch:12, best blue:0.2423\n",
            "cur epoch:32, cur blue:0.2280, best epoch:12, best blue:0.2423\n",
            "cur epoch:33, cur blue:0.1971, best epoch:12, best blue:0.2423\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X64q36fSOwQw"
      },
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/chatbot/ULMFIT_Calibration/Glove/best_chatbot_models_state_dict.pth'))\n",
        "ece_criterion = _ECELoss().to(device)\n",
        "bleu, logits_all, labels_all = evaluation(model, test_loader)\n",
        "logits_all = logits_all.view(-1,ntokens)\n",
        "labels_all = labels_all.view(-1)\n",
        "temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
        "\n",
        "evaluate_matrics(model,test_loader)\n",
        "temperature_ece"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}